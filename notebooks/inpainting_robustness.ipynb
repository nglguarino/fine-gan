{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "authorship_tag": "ABX9TyPJnzyBRKebY3Iw6MxS2Iq4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QEPQC0_tNEas",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754631811705,
     "user_tz": -120,
     "elapsed": 24225,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    },
    "outputId": "6c184bee-430d-4995-d6de-12c433fd93ec"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Unzip the dataset ---\n",
    "# IMPORTANT: Update this path to match the location of your zip file in Drive.\n",
    "zip_path = '/content/drive/MyDrive/ColabNotebooks/Vision/inpainting/ffhq256_10ksubset.zip'\n",
    "\n",
    "# The destination folder in the local Colab environment.\n",
    "destination_path = '/content'\n",
    "\n",
    "print(\"Unzipping dataset...\")\n",
    "# The -q flag makes the output cleaner (quiet mode)\n",
    "!unzip -q {zip_path} -d {destination_path}\n",
    "\n",
    "print(f\"✅ Dataset unzipped to {destination_path}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "93abx4XMNHvE",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754631855008,
     "user_tz": -120,
     "elapsed": 43288,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    },
    "outputId": "1c3dad92-05b2-4aa9-cc0e-8f35bcd7766e"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unzipping dataset...\n",
      "✅ Dataset unzipped to /content\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Install\n",
    "!pip -q install pytorch_wavelets torchmetrics lpips torch-fidelity"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QqeS8vZKNNeQ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754631954633,
     "user_tz": -120,
     "elapsed": 99660,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    },
    "outputId": "b9556132-eecb-4360-c3c6-d8ec00837609"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/54.9 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m54.9/54.9 kB\u001B[0m \u001B[31m4.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/983.0 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m983.0/983.0 kB\u001B[0m \u001B[31m47.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m53.8/53.8 kB\u001B[0m \u001B[31m4.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m363.4/363.4 MB\u001B[0m \u001B[31m4.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m13.8/13.8 MB\u001B[0m \u001B[31m94.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m24.6/24.6 MB\u001B[0m \u001B[31m64.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m883.7/883.7 kB\u001B[0m \u001B[31m52.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m664.8/664.8 MB\u001B[0m \u001B[31m2.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m211.5/211.5 MB\u001B[0m \u001B[31m7.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m56.3/56.3 MB\u001B[0m \u001B[31m12.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m127.9/127.9 MB\u001B[0m \u001B[31m7.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m207.5/207.5 MB\u001B[0m \u001B[31m6.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m188.7/188.7 MB\u001B[0m \u001B[31m5.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m21.1/21.1 MB\u001B[0m \u001B[31m43.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch._dynamo\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.nn.utils import spectral_norm\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm.autonotebook import tqdm\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchmetrics\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "import lpips\n",
    "\n",
    "import torch.fft\n",
    "from pytorch_wavelets import DWTForward\n",
    "from einops import rearrange\n",
    "\n",
    "# Enable TensorFloat32\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# --- Configuration & Hyperparameters ---\n",
    "\n",
    "# Set the path to your image folder in Google Drive\n",
    "# IMPORTANT: Update this path to match where you saved your dataset.\n",
    "DATASET_PATH = '/content/ffhq_subset_10k'\n",
    "NUM_IMAGES_TO_USE_CNN = 10000\n",
    "NUM_IMAGES_TO_USE_GAN = 1000\n",
    "\n",
    "# Training settings\n",
    "NUM_EPOCHS_CNN = 40\n",
    "NUM_EPOCHS_GAN = 200\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# Set the device (use GPU if available)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a081D9HKNQaH",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754631977173,
     "user_tz": -120,
     "elapsed": 22544,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    },
    "outputId": "f485194d-96a2-4003-f82b-08620d44baeb"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class FFHQDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset for loading FFHQ images.\"\"\"\n",
    "    def __init__(self, img_dir, transform=None, num_images=None): # Add num_images parameter\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            num_images (int, optional): Number of images to use. If None, use all images.\n",
    "        \"\"\"\n",
    "        # Find all files with .png or .jpg extension\n",
    "        self.img_paths = glob.glob(os.path.join(img_dir, '*.png'))\n",
    "        self.img_paths.extend(glob.glob(os.path.join(img_dir, '*.jpg')))\n",
    "        self.transform = transform\n",
    "\n",
    "        if num_images:\n",
    "            # If a number is specified, shuffle all paths and take a random subset\n",
    "            random.shuffle(self.img_paths)\n",
    "            self.img_paths = self.img_paths[:num_images]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "# Your transform definition remains the same\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ],
   "metadata": {
    "id": "vCTiHzArNWCB",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754631977175,
     "user_tz": -120,
     "elapsed": 85,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\" A simple self-attention layer \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.query = nn.Conv2d(channels, channels // 8, 1)\n",
    "        self.key   = nn.Conv2d(channels, channels // 8, 1)\n",
    "        self.value = nn.Conv2d(channels, channels, 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, C, width, height = x.size()\n",
    "        q = self.query(x).view(batch_size, -1, width * height).permute(0, 2, 1)\n",
    "        k = self.key(x).view(batch_size, -1, width * height)\n",
    "        v = self.value(x).view(batch_size, -1, width * height)\n",
    "\n",
    "        attention_map = F.softmax(torch.bmm(q, k), dim=-1)\n",
    "\n",
    "        out = torch.bmm(v, attention_map.permute(0, 2, 1))\n",
    "        out = out.view(batch_size, C, width, height)\n",
    "\n",
    "        return self.gamma * out + x # Add skip connection\n",
    "\n",
    "class UpsampleBlock(nn.Module):\n",
    "    \"\"\"An upsampling block using Conv2d and PixelShuffle.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # The Conv2d layer produces 4x the channels for a 2x upscale\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels * 4, kernel_size=3, padding=1)\n",
    "        self.pixel_shuffle = nn.PixelShuffle(2) # Rearranges channels to upscale by 2x\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.pixel_shuffle(self.conv(x)))\n",
    "\n",
    "class GatedConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    A Gated Convolutional Layer.\n",
    "    It learns a dynamic feature mask for each channel at every location.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1):\n",
    "        super().__init__()\n",
    "        # Convolution for the features\n",
    "        self.conv_feature = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size, stride, padding, dilation\n",
    "        )\n",
    "        # Convolution for the gating mechanism\n",
    "        self.conv_gate = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size, stride, padding, dilation\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get the features and the gate\n",
    "        features = self.conv_feature(x)\n",
    "        gate = torch.sigmoid(self.conv_gate(x)) # Gate values are between 0 and 1\n",
    "\n",
    "        # Element-wise multiplication to apply the learned gate\n",
    "        return features * gate\n",
    "\n",
    "class GatedResidualBlock(nn.Module):\n",
    "    \"\"\"A Residual Block that uses Gated Convolutions.\"\"\"\n",
    "    def __init__(self, channels, dilation=1):\n",
    "        super().__init__()\n",
    "        padding = dilation\n",
    "\n",
    "        # Replace nn.Conv2d with GatedConv2d\n",
    "        self.conv1 = GatedConv2d(channels, channels, kernel_size=3, padding=padding, dilation=dilation)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = GatedConv2d(channels, channels, kernel_size=3, padding=padding, dilation=dilation)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = out + residual  # Residual connection\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class UNetSR(nn.Module):\n",
    "    \"\"\"\n",
    "    A U-Net architecture with corrected channel dimensions for the decoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=4, out_channels=3, num_channels=64):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Initial Convolution ---\n",
    "        self.init_conv = nn.Conv2d(in_channels, num_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        # --- Encoder Path ---\n",
    "        self.enc1 = GatedResidualBlock(num_channels, dilation=1)\n",
    "        self.enc2 = GatedResidualBlock(num_channels, dilation=1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # --- Bottleneck with Dilation and Attention ---\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            GatedResidualBlock(num_channels, dilation=2),\n",
    "            # SelfAttention(num_channels), # Add attention layer\n",
    "            GatedResidualBlock(num_channels, dilation=4)\n",
    "        )\n",
    "\n",
    "        # --- Decoder Path ---\n",
    "        self.upconv2 = UpsampleBlock(num_channels, num_channels)\n",
    "        # Input channels = upsampled (64) + skip connection from e2 (64) = 128\n",
    "        self.dec2 = GatedResidualBlock(num_channels * 2, dilation=1)\n",
    "\n",
    "        self.upconv1 = UpsampleBlock(num_channels * 2, num_channels)\n",
    "        # Input channels = upsampled (64) + skip connection from e1 (64) = 128\n",
    "        self.dec1 = GatedResidualBlock(num_channels * 2, dilation=1)\n",
    "\n",
    "        # --- Final Output Layer ---\n",
    "        # The input to this layer comes from dec1, which outputs 128 channels.\n",
    "        self.out_conv = nn.Conv2d(num_channels * 2, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial feature extraction\n",
    "        x0 = self.init_conv(x)\n",
    "\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x0)\n",
    "        p1 = self.pool(e1)\n",
    "\n",
    "        e2 = self.enc2(p1)\n",
    "        p2 = self.pool(e2)\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(p2)\n",
    "\n",
    "        # Decoder with Skip Connections\n",
    "        d2 = self.upconv2(b)\n",
    "        d2 = torch.cat([d2, e2], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "\n",
    "        d1 = self.upconv1(d2)\n",
    "        d1 = torch.cat([d1, e1], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "\n",
    "        # Final Output\n",
    "        out = self.out_conv(d1)\n",
    "\n",
    "        return torch.tanh(out)"
   ],
   "metadata": {
    "id": "v12E0ImxNf7S",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754631977195,
     "user_tz": -120,
     "elapsed": 70,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# --- 1. Create the main dataset ---\n",
    "# This should use the LARGER number of images you intend to work with.\n",
    "# Let's assume NUM_IMAGES_TO_USE_CNN is the total pool of images.\n",
    "print(\"Creating the main dataset...\")\n",
    "full_dataset = FFHQDataset(\n",
    "    img_dir=DATASET_PATH,\n",
    "    transform=transform,\n",
    "    num_images=NUM_IMAGES_TO_USE_CNN # Use the total number of images available for the experiment\n",
    ")\n",
    "print(f\"✅ Main dataset created with {len(full_dataset)} images.\")\n",
    "\n",
    "# --- 2. Split the dataset into Training, Validation, and Test sets ---\n",
    "print(\"\\nSplitting data into training, validation, and test sets...\")\n",
    "dataset_size = len(full_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "np.random.seed(42) # for reproducibility\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Define split points for an 80/10/10 split\n",
    "train_split = int(np.floor(0.8 * dataset_size))\n",
    "val_split = int(np.floor(0.9 * dataset_size))\n",
    "\n",
    "# Create indices for each set\n",
    "train_indices = indices[:train_split]\n",
    "val_indices = indices[train_split:val_split]\n",
    "test_indices = indices[val_split:]\n",
    "\n",
    "# Create PyTorch Subsets\n",
    "train_data = Subset(full_dataset, train_indices)\n",
    "val_data = Subset(full_dataset, val_indices)\n",
    "test_data = Subset(full_dataset, test_indices)\n",
    "\n",
    "print(f\"✅ Training set size: {len(train_data)}\")\n",
    "print(f\"✅ Validation set size: {len(val_data)}\")\n",
    "print(f\"✅ Test set size: {len(test_data)}\")\n",
    "\n",
    "# --- 3. Create the Diffusion Model's training subset ---\n",
    "# This should be a subset of the TRAINING data.\n",
    "print(\"\\nCreating a subset of the training data for the GAN model...\")\n",
    "gan_indices = train_indices[:NUM_IMAGES_TO_USE_GAN] # Take from the start of shuffled train indices\n",
    "gan_data = Subset(full_dataset, gan_indices)\n",
    "print(f\"✅ Diffusion training set size: {len(gan_data)}\")\n",
    "\n",
    "\n",
    "# --- 4. Create DataLoaders for each set ---\n",
    "print(\"\\nCreating DataLoaders...\")\n",
    "# The main CNN will now train on the 'train_data' subset\n",
    "cnn_dataloader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "# New DataLoader for validation\n",
    "val_dataloader = DataLoader(\n",
    "    val_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False, # No need to shuffle validation data\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "# New DataLoader for testing\n",
    "test_dataloader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False, # No need to shuffle test data\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "# GAN DataLoader uses its own subset of the training data\n",
    "gan_dataloader = DataLoader(\n",
    "    gan_data,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "print(\"✅ All DataLoaders created.\")\n",
    "\n",
    "# --- 5. Initialize models and optimizers (unchanged) ---\n",
    "print(\"\\nInitializing models and optimizers...\")\n",
    "cnn_model = UNetSR().to(DEVICE)\n",
    "\n",
    "# Compile the models for a speed boost\n",
    "cnn_model = torch.compile(cnn_model)\n",
    "\n",
    "# Initialize optimizers\n",
    "optimizer_cnn = optim.Adam(cnn_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"\\nSetup complete. Ready for CNN training!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m8yRHy5TNjPW",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754631980528,
     "user_tz": -120,
     "elapsed": 3331,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    },
    "outputId": "4d70f043-0f9d-444d-f3e6-4ad7edeba0e0"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Creating the main dataset...\n",
      "✅ Main dataset created with 10000 images.\n",
      "\n",
      "Splitting data into training, validation, and test sets...\n",
      "✅ Training set size: 8000\n",
      "✅ Validation set size: 1000\n",
      "✅ Test set size: 1000\n",
      "\n",
      "Creating a subset of the training data for the GAN model...\n",
      "✅ Diffusion training set size: 1000\n",
      "\n",
      "Creating DataLoaders...\n",
      "✅ All DataLoaders created.\n",
      "\n",
      "Initializing models and optimizers...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Setup complete. Ready for CNN training!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "hz1lZw57MpDC",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754632273099,
     "user_tz": -120,
     "elapsed": 46,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    }
   },
   "outputs": [],
   "source": [
    "def create_variable_mask(images, mask_percentage):\n",
    "    \"\"\"\n",
    "    Create masks with specified percentage of image area\n",
    "    Args:\n",
    "        images: [B, C, H, W] tensor\n",
    "        mask_percentage: float between 0 and 1 (e.g., 0.025 for 2.5%)\n",
    "    \"\"\"\n",
    "    batch_size, _, height, width = images.shape\n",
    "    masks = torch.ones_like(images)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Calculate mask size\n",
    "        total_pixels = height * width\n",
    "        mask_pixels = int(total_pixels * mask_percentage)\n",
    "\n",
    "        # Create random mask (you can also use square/irregular masks)\n",
    "        if mask_percentage < 0.1:  # For small masks, use squares\n",
    "            # Square mask\n",
    "            mask_size = int(np.sqrt(mask_pixels))\n",
    "            top = np.random.randint(0, height - mask_size)\n",
    "            left = np.random.randint(0, width - mask_size)\n",
    "            masks[i, :, top:top+mask_size, left:left+mask_size] = 0\n",
    "        else:  # For larger masks, use irregular shapes\n",
    "            # Multiple rectangles for more realistic patterns\n",
    "            num_rects = np.random.randint(1, 5)\n",
    "            pixels_per_rect = mask_pixels // num_rects\n",
    "\n",
    "            for _ in range(num_rects):\n",
    "                rect_h = np.random.randint(int(np.sqrt(pixels_per_rect)//2),\n",
    "                                          min(int(np.sqrt(pixels_per_rect)*2), height//2))\n",
    "                rect_w = pixels_per_rect // rect_h\n",
    "\n",
    "                if rect_h < height and rect_w < width:\n",
    "                    top = np.random.randint(0, max(1, height - rect_h))\n",
    "                    left = np.random.randint(0, max(1, width - rect_w))\n",
    "                    masks[i, :, top:min(top+rect_h, height), left:min(left+rect_w, width)] = 0\n",
    "\n",
    "    masked_images = images * masks\n",
    "    return masked_images, masks\n",
    "\n",
    "def evaluate_cnn_multiple_masks(cnn_model, test_loader, mask_percentages, device='cuda', num_batches=None):\n",
    "    \"\"\"\n",
    "    Evaluate CNN performance with different mask sizes\n",
    "\n",
    "    Args:\n",
    "        cnn_model: Your trained CNN model\n",
    "        test_loader: DataLoader with test images\n",
    "        mask_percentages: List of mask percentages to test (e.g., [0.025, 0.05, 0.1, 0.2, 0.4])\n",
    "        device: cuda or cpu\n",
    "        num_batches: Number of batches to evaluate (None for all)\n",
    "    \"\"\"\n",
    "\n",
    "    cnn_model.eval()\n",
    "    results = {pct: {'psnr': [], 'ssim': [], 'lpips': [], 'mae': [],\n",
    "                     'psnr_hole': [], 'mae_hole': []}\n",
    "               for pct in mask_percentages}\n",
    "\n",
    "    # Initialize metrics\n",
    "    psnr_metric = torchmetrics.PeakSignalNoiseRatio(data_range=1.0).to(device)\n",
    "    ssim_metric = torchmetrics.StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "    lpips_metric = lpips.LPIPS(net='alex').to(device)\n",
    "\n",
    "    print(\"Evaluating CNN with varying mask sizes...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for mask_pct in mask_percentages:\n",
    "        print(f\"\\nTesting with {mask_pct*100:.1f}% mask...\")\n",
    "\n",
    "        batch_count = 0\n",
    "        for batch_idx, batch in enumerate(tqdm(test_loader, desc=f\"Mask {mask_pct*100:.1f}%\")):\n",
    "            if num_batches and batch_idx >= num_batches:\n",
    "                break\n",
    "\n",
    "            original_images = batch.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Create masks with current percentage\n",
    "                masked_images, masks = create_variable_mask(original_images, mask_pct)\n",
    "                masks = masks.to(device)\n",
    "\n",
    "                # CNN inference\n",
    "                cnn_input = torch.cat([masked_images, masks[:, 0:1]], dim=1)\n",
    "                cnn_output = cnn_model(cnn_input)\n",
    "\n",
    "                # Ensure output is in proper range\n",
    "                cnn_output = torch.clamp(cnn_output, -1, 1)\n",
    "\n",
    "                # Convert to [0, 1] for metrics\n",
    "                original_01 = (original_images + 1) / 2\n",
    "                output_01 = (cnn_output + 1) / 2\n",
    "\n",
    "                # === WHOLE IMAGE METRICS ===\n",
    "                results[mask_pct]['psnr'].append(psnr_metric(output_01, original_01).item())\n",
    "                results[mask_pct]['ssim'].append(ssim_metric(output_01, original_01).item())\n",
    "                results[mask_pct]['lpips'].append(lpips_metric(cnn_output, original_images).mean().item())\n",
    "                results[mask_pct]['mae'].append(F.l1_loss(output_01, original_01).item())\n",
    "\n",
    "                # === HOLE-ONLY METRICS ===\n",
    "                hole_mask = 1 - masks[:, 0:1]\n",
    "                if hole_mask.sum() > 0:\n",
    "                    # PSNR for inpainted region only\n",
    "                    mse_hole = ((output_01 * hole_mask - original_01 * hole_mask) ** 2).sum() / hole_mask.sum()\n",
    "                    psnr_hole = 20 * torch.log10(torch.tensor(1.0)) - 10 * torch.log10(mse_hole)\n",
    "                    results[mask_pct]['psnr_hole'].append(psnr_hole.item())\n",
    "\n",
    "                    # MAE for inpainted region only\n",
    "                    mae_hole = (F.l1_loss(output_01 * hole_mask, original_01 * hole_mask, reduction='sum') / hole_mask.sum()).item()\n",
    "                    results[mask_pct]['mae_hole'].append(mae_hole)\n",
    "                else:\n",
    "                    results[mask_pct]['psnr_hole'].append(results[mask_pct]['psnr'][-1])\n",
    "                    results[mask_pct]['mae_hole'].append(results[mask_pct]['mae'][-1])\n",
    "\n",
    "            batch_count += 1\n",
    "\n",
    "    return results\n",
    "\n",
    "def print_results_table(results):\n",
    "    \"\"\"Print results in a nice table format\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CNN PERFORMANCE VS MASK SIZE - WHOLE IMAGE METRICS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Prepare data for whole image metrics\n",
    "    data_whole = []\n",
    "    for mask_pct, metrics in results.items():\n",
    "        data_whole.append({\n",
    "            'Mask %': f\"{mask_pct*100:.1f}%\",\n",
    "            'PSNR ↑': f\"{np.mean(metrics['psnr']):.2f} ± {np.std(metrics['psnr']):.2f}\",\n",
    "            'SSIM ↑': f\"{np.mean(metrics['ssim']):.4f} ± {np.std(metrics['ssim']):.4f}\",\n",
    "            'LPIPS ↓': f\"{np.mean(metrics['lpips']):.4f} ± {np.std(metrics['lpips']):.4f}\",\n",
    "            'MAE ↓': f\"{np.mean(metrics['mae']):.4f} ± {np.std(metrics['mae']):.4f}\",\n",
    "        })\n",
    "\n",
    "    df_whole = pd.DataFrame(data_whole)\n",
    "    print(df_whole.to_string(index=False))\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CNN PERFORMANCE VS MASK SIZE - INPAINTED REGION ONLY\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Prepare data for hole metrics\n",
    "    data_hole = []\n",
    "    for mask_pct, metrics in results.items():\n",
    "        data_hole.append({\n",
    "            'Mask %': f\"{mask_pct*100:.1f}%\",\n",
    "            'PSNR (hole) ↑': f\"{np.mean(metrics['psnr_hole']):.2f} ± {np.std(metrics['psnr_hole']):.2f}\",\n",
    "            'MAE (hole) ↓': f\"{np.mean(metrics['mae_hole']):.4f} ± {np.std(metrics['mae_hole']):.4f}\",\n",
    "        })\n",
    "\n",
    "    df_hole = pd.DataFrame(data_hole)\n",
    "    print(df_hole.to_string(index=False))\n",
    "\n",
    "def create_performance_plots(results, save_path='cnn_mask_performance.png'):\n",
    "    \"\"\"Create visualization of performance vs mask size\"\"\"\n",
    "\n",
    "    mask_sizes = sorted(results.keys())\n",
    "    mask_percentages = [m * 100 for m in mask_sizes]\n",
    "\n",
    "    # Extract mean values\n",
    "    psnr_means = [np.mean(results[m]['psnr']) for m in mask_sizes]\n",
    "    psnr_stds = [np.std(results[m]['psnr']) for m in mask_sizes]\n",
    "\n",
    "    ssim_means = [np.mean(results[m]['ssim']) for m in mask_sizes]\n",
    "    ssim_stds = [np.std(results[m]['ssim']) for m in mask_sizes]\n",
    "\n",
    "    lpips_means = [np.mean(results[m]['lpips']) for m in mask_sizes]\n",
    "    lpips_stds = [np.std(results[m]['lpips']) for m in mask_sizes]\n",
    "\n",
    "    psnr_hole_means = [np.mean(results[m]['psnr_hole']) for m in mask_sizes]\n",
    "    psnr_hole_stds = [np.std(results[m]['psnr_hole']) for m in mask_sizes]\n",
    "\n",
    "    # Create plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "    # PSNR vs Mask Size\n",
    "    axes[0, 0].errorbar(mask_percentages, psnr_means, yerr=psnr_stds,\n",
    "                       marker='o', capsize=5, capthick=2, linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Mask Size (%)')\n",
    "    axes[0, 0].set_ylabel('PSNR (dB)')\n",
    "    axes[0, 0].set_title('PSNR vs Mask Size (Whole Image)')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # SSIM vs Mask Size\n",
    "    axes[0, 1].errorbar(mask_percentages, ssim_means, yerr=ssim_stds,\n",
    "                       marker='s', capsize=5, capthick=2, linewidth=2, color='green')\n",
    "    axes[0, 1].set_xlabel('Mask Size (%)')\n",
    "    axes[0, 1].set_ylabel('SSIM')\n",
    "    axes[0, 1].set_title('SSIM vs Mask Size (Whole Image)')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # LPIPS vs Mask Size\n",
    "    axes[1, 0].errorbar(mask_percentages, lpips_means, yerr=lpips_stds,\n",
    "                       marker='^', capsize=5, capthick=2, linewidth=2, color='red')\n",
    "    axes[1, 0].set_xlabel('Mask Size (%)')\n",
    "    axes[1, 0].set_ylabel('LPIPS (lower is better)')\n",
    "    axes[1, 0].set_title('LPIPS vs Mask Size')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # PSNR Hole vs Mask Size\n",
    "    axes[1, 1].errorbar(mask_percentages, psnr_hole_means, yerr=psnr_hole_stds,\n",
    "                       marker='d', capsize=5, capthick=2, linewidth=2, color='purple')\n",
    "    axes[1, 1].set_xlabel('Mask Size (%)')\n",
    "    axes[1, 1].set_ylabel('PSNR (dB)')\n",
    "    axes[1, 1].set_title('PSNR vs Mask Size (Inpainted Region Only)')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle('CNN Inpainting Performance vs Mask Size', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nPlots saved to {save_path}\")\n",
    "\n",
    "def create_visual_comparison(cnn_model, test_loader, mask_percentages, device='cuda',\n",
    "                            save_path='mask_size_visual_comparison.png'):\n",
    "    \"\"\"Create visual comparison showing results with different mask sizes\"\"\"\n",
    "\n",
    "    # Get one sample image\n",
    "    sample_batch = next(iter(test_loader))\n",
    "    sample_image = sample_batch[0:1].to(device)\n",
    "\n",
    "    fig, axes = plt.subplots(3, len(mask_percentages), figsize=(4*len(mask_percentages), 12))\n",
    "\n",
    "    cnn_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, mask_pct in enumerate(mask_percentages):\n",
    "            # Create mask\n",
    "            masked_image, mask = create_variable_mask(sample_image, mask_pct)\n",
    "            mask = mask.to(device)\n",
    "\n",
    "            # CNN inference\n",
    "            cnn_input = torch.cat([masked_image, mask[:, 0:1]], dim=1)\n",
    "            cnn_output = cnn_model(cnn_input)\n",
    "\n",
    "            # Convert to displayable format\n",
    "            def to_img(tensor):\n",
    "                return ((tensor[0].cpu() + 1) / 2).clamp(0, 1).permute(1, 2, 0)\n",
    "\n",
    "            # Row 1: Masked input\n",
    "            axes[0, idx].imshow(to_img(masked_image))\n",
    "            axes[0, idx].set_title(f'Masked ({mask_pct*100:.1f}%)')\n",
    "            axes[0, idx].axis('off')\n",
    "\n",
    "            # Row 2: CNN output\n",
    "            axes[1, idx].imshow(to_img(cnn_output))\n",
    "            axes[1, idx].set_title('CNN Output')\n",
    "            axes[1, idx].axis('off')\n",
    "\n",
    "            # Row 3: Error map\n",
    "            error = torch.abs(cnn_output - sample_image).mean(dim=1, keepdim=True)\n",
    "            axes[2, idx].imshow(error[0, 0].cpu(), cmap='hot', vmin=0, vmax=0.5)\n",
    "            axes[2, idx].set_title('Error Map')\n",
    "            axes[2, idx].axis('off')\n",
    "\n",
    "    # Add row labels\n",
    "    axes[0, 0].set_ylabel('Masked Input', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('CNN Output', fontsize=12, fontweight='bold')\n",
    "    axes[2, 0].set_ylabel('Error Map', fontsize=12, fontweight='bold')\n",
    "\n",
    "    plt.suptitle('CNN Performance Across Different Mask Sizes', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Visual comparison saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ==========================================\n",
    "# MAIN EXECUTION\n",
    "# ==========================================\n",
    "\n",
    "# Define mask percentages to test\n",
    "MASK_PERCENTAGES = [0.025, 0.05, 0.1, 0.2]  # 2.5%, 5%, 10%, 20%\n",
    "\n",
    "# Load your CNN model\n",
    "print(\"Loading CNN model...\")\n",
    "cnn_model = UNetSR(in_channels=4, out_channels=3).to(DEVICE)\n",
    "cnn_state_dict = torch.load('/content/drive/MyDrive/ColabNotebooks/Vision/inpainting/final_cnn_model.pth', map_location=DEVICE)\n",
    "if any(k.startswith('_orig_mod.') for k in cnn_state_dict.keys()):\n",
    "    cnn_state_dict = {k.replace('_orig_mod.', ''): v for k, v in cnn_state_dict.items()}\n",
    "cnn_model.load_state_dict(cnn_state_dict)\n",
    "cnn_model.eval()\n",
    "print(\"✅ CNN model loaded successfully\")\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluate_cnn_multiple_masks(\n",
    "    cnn_model=cnn_model,\n",
    "    test_loader=test_dataloader,\n",
    "    mask_percentages=MASK_PERCENTAGES,\n",
    "    device=DEVICE,\n",
    "    num_batches=None  # Use 50 batches for faster testing, or None for full test set\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print_results_table(results)\n",
    "\n",
    "# Create performance plots\n",
    "create_performance_plots(results, save_path='cnn_mask_performance.png')\n",
    "\n",
    "# Create visual comparison\n",
    "create_visual_comparison(\n",
    "    cnn_model=cnn_model,\n",
    "    test_loader=test_dataloader,\n",
    "    mask_percentages=MASK_PERCENTAGES,\n",
    "    device=DEVICE,\n",
    "    save_path='mask_size_visual_comparison.png'\n",
    ")\n",
    "\n",
    "# Save results to CSV for further analysis\n",
    "print(\"\\nSaving detailed results to CSV...\")\n",
    "detailed_results = []\n",
    "for mask_pct in MASK_PERCENTAGES:\n",
    "    for i in range(len(results[mask_pct]['psnr'])):\n",
    "        detailed_results.append({\n",
    "            'mask_percentage': mask_pct * 100,\n",
    "            'psnr': results[mask_pct]['psnr'][i],\n",
    "            'ssim': results[mask_pct]['ssim'][i],\n",
    "            'lpips': results[mask_pct]['lpips'][i],\n",
    "            'mae': results[mask_pct]['mae'][i],\n",
    "            'psnr_hole': results[mask_pct]['psnr_hole'][i],\n",
    "            'mae_hole': results[mask_pct]['mae_hole'][i],\n",
    "        })\n",
    "\n",
    "df_detailed = pd.DataFrame(detailed_results)\n",
    "df_detailed.to_csv('cnn_mask_evaluation_detailed.csv', index=False)\n",
    "print(\"✅ Results saved to cnn_mask_evaluation_detailed.csv\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Tested mask sizes: {[f'{m*100:.1f}%' for m in MASK_PERCENTAGES]}\")\n",
    "print(f\"Number of test batches: {len(results[MASK_PERCENTAGES[0]]['psnr'])}\")\n",
    "print(\"\\nKey Findings:\")\n",
    "for mask_pct in MASK_PERCENTAGES:\n",
    "    psnr_mean = np.mean(results[mask_pct]['psnr'])\n",
    "    print(f\"  {mask_pct*100:4.1f}% mask → PSNR: {psnr_mean:.2f} dB\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "7906ca72c5f84302b21686ee609ece7b",
      "5c86c0d1d9614b3abc19228b59fb013e",
      "9fe849405eef44bfa0e88ca4087e0381",
      "ae5a3e20e2884137bf9345f2b2b5d279",
      "d769fb5a25da46829842a809f3a8dd19",
      "df08640f63ee46dca23c0205d8ef7ce4",
      "ad81106a1c83449ea83e960f82c50e91",
      "d12fbb3f23ee40b9842d0c4585236f38",
      "e6968e8895bb44eb9196a6bbd878b4d2",
      "83f0887679054a8d8f3037026aad8f69",
      "a8877fd0df37493ea696add6b4cb8e7e",
      "ca5fa3f36f7849e9aeae09c710cf0e11",
      "8499aad3abfb45a395b378290a65443b",
      "c3edaa1809d840299c9a48fecacc76df",
      "a1763a6c7d7b46c1acc11270ad56a110",
      "6676811f24ff4509b2481d92fe0e55fa",
      "cc46f9cca4784a788f81d05a0d581fa4",
      "d4502382ff3347979198f6dabf80bd5b",
      "e4eb20330dcc4d1f854b976f881f1659",
      "2a3f131eefdb4c19b0d226e078e19033",
      "95b2e5662891492d9487fe4eee70f69d",
      "1382afea24e54d81a3ed78122411b2af",
      "a1e303b3718746d9ad5a5554e2f11744",
      "c17e158fccff48b6a582319da6f5e97d",
      "f6233e698af341ac878d16513f596ca6",
      "db4d1b8fb2f544f7b292838269b6c70f",
      "f1c2f32eed154cc99147b29619638245",
      "f0367a88cdb243b0a1f579ecf5d71e01",
      "4bded63f6e3b439d8e5cad30226a1707",
      "5c8c6ff235b0480fbb816664fee311e7",
      "c7fe25857766434b8cb83bd561c3ed6c",
      "455b1fa65b6c4af2befcdac43a65cb37",
      "5b0844f40e9a4e2abeece8b2d6a52dd7",
      "ef7750b8ea1f43948d7b4c7101af5b9a",
      "171191129d7344a4a99cfe74d20ffddf",
      "e8591e4e831540719db36b5920b83af7",
      "614fac4392954b31aba91dad39efd0ea",
      "e47d34fb2d9a4594943d36901b38cd4b",
      "a8f920e0fbd548ae950c8a40d3080af5",
      "0e7d0ab90e014ba5b0dc0f79fef41db5",
      "2bc2171bbc0b41518107543aa3f74cdb",
      "8ec291afd1284f93a3bb4f698e7dd032",
      "b2af50db9b454056af33973347c6bd6b",
      "696d5d91b47b454b85fe23563210b212"
     ]
    },
    "id": "woG6cMXCMxPW",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754632420420,
     "user_tz": -120,
     "elapsed": 140354,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    },
    "outputId": "215e6529-a062-44e0-ddb7-a1131a384bb6"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
