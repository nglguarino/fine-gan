{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "L4",
   "authorship_tag": "ABX9TyOp/KVJzq6GQF5UG006uhFa"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KrVJBiV8wXEw",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754624550824,
     "user_tz": -120,
     "elapsed": 21016,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    },
    "outputId": "1285cc82-586e-4a9a-e624-f94dd1a6ea26"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Unzip the dataset ---\n",
    "# IMPORTANT: Update this path to match the location of your zip file in Drive.\n",
    "zip_path = '/content/drive/MyDrive/ColabNotebooks/Vision/inpainting/ffhq256_10ksubset.zip'\n",
    "\n",
    "# The destination folder in the local Colab environment\n",
    "destination_path = '/content'\n",
    "\n",
    "print(\"Unzipping dataset...\")\n",
    "# The -q flag makes the output cleaner (quiet mode)\n",
    "!unzip -q {zip_path} -d {destination_path}\n",
    "\n",
    "print(f\"âœ… Dataset unzipped to {destination_path}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mkGfy_R6wcLX",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754624584941,
     "user_tz": -120,
     "elapsed": 34110,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    },
    "outputId": "6d5c679d-aa5c-4890-a360-28c4b2ec9622"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unzipping dataset...\n",
      "âœ… Dataset unzipped to /content\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Install\n",
    "!pip -q install pytorch_wavelets torchmetrics lpips torch-fidelity"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ehat2EAgweJd",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754624670032,
     "user_tz": -120,
     "elapsed": 85092,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    },
    "outputId": "0fa6f650-588d-4a22-99a7-8debe6c7f31b"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[?25l   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m0.0/54.9 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m54.9/54.9 kB\u001B[0m \u001B[31m5.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h\u001B[?25l   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m0.0/983.0 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m983.0/983.0 kB\u001B[0m \u001B[31m57.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m53.8/53.8 kB\u001B[0m \u001B[31m5.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m363.4/363.4 MB\u001B[0m \u001B[31m2.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m13.8/13.8 MB\u001B[0m \u001B[31m90.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m24.6/24.6 MB\u001B[0m \u001B[31m90.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m883.7/883.7 kB\u001B[0m \u001B[31m50.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m664.8/664.8 MB\u001B[0m \u001B[31m1.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m211.5/211.5 MB\u001B[0m \u001B[31m3.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m56.3/56.3 MB\u001B[0m \u001B[31m37.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m127.9/127.9 MB\u001B[0m \u001B[31m16.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m207.5/207.5 MB\u001B[0m \u001B[31m5.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m188.7/188.7 MB\u001B[0m \u001B[31m4.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m21.1/21.1 MB\u001B[0m \u001B[31m102.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch._dynamo\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.nn.utils import spectral_norm\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm.autonotebook import tqdm\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchmetrics\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "import lpips\n",
    "\n",
    "import torch.fft\n",
    "from pytorch_wavelets import DWTForward\n",
    "from einops import rearrange\n",
    "\n",
    "# Enable TensorFloat32\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# --- Configuration & Hyperparameters ---\n",
    "\n",
    "# Set the path to your image folder in Google Drive\n",
    "# IMPORTANT: Update this path to match where you saved your dataset.\n",
    "DATASET_PATH = '/content/ffhq_subset_10k'\n",
    "NUM_IMAGES_TO_USE_CNN = 10000\n",
    "NUM_IMAGES_TO_USE_GAN = 1000\n",
    "\n",
    "# Training settings\n",
    "NUM_EPOCHS_CNN = 40\n",
    "NUM_EPOCHS_GAN = 200\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# Set the device (use GPU if available)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nzuOe6RCwgcX",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754624682985,
     "user_tz": -120,
     "elapsed": 12956,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    },
    "outputId": "36755721-3b17-46d6-b8c7-07b398a990f1"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class FFHQDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset for loading FFHQ images.\"\"\"\n",
    "    def __init__(self, img_dir, transform=None, num_images=None): # Add num_images parameter\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            num_images (int, optional): Number of images to use. If None, use all images.\n",
    "        \"\"\"\n",
    "        # Find all files with .png or .jpg extension\n",
    "        self.img_paths = glob.glob(os.path.join(img_dir, '*.png'))\n",
    "        self.img_paths.extend(glob.glob(os.path.join(img_dir, '*.jpg')))\n",
    "        self.transform = transform\n",
    "\n",
    "        if num_images:\n",
    "            # If a number is specified, shuffle all paths and take a random subset\n",
    "            random.shuffle(self.img_paths)\n",
    "            self.img_paths = self.img_paths[:num_images]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "# Your transform definition remains the same\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ],
   "metadata": {
    "id": "FWTDVVFDwiju",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754624682991,
     "user_tz": -120,
     "elapsed": 9,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def tensor_to_image(tensor):\n",
    "    \"\"\"\n",
    "    Converts a PyTorch tensor to a displayable NumPy image.\n",
    "    It denormalizes, moves to CPU, and changes dimension order.\n",
    "    \"\"\"\n",
    "    # Denormalize the image from [-1, 1] to [0, 1]\n",
    "    image = tensor * 0.5 + 0.5\n",
    "    # Move tensor to CPU and convert to NumPy array\n",
    "    image = image.cpu().numpy()\n",
    "    # Transpose dimensions from (C, H, W) to (H, W, C) for plotting\n",
    "    image = image.transpose(1, 2, 0)\n",
    "    # Clip values to be in the valid [0, 1] range for images\n",
    "    image = np.clip(image, 0, 1)\n",
    "    return image"
   ],
   "metadata": {
    "id": "k4reTFP5wmQS",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754624683014,
     "user_tz": -120,
     "elapsed": 3,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def create_mask(image, mask_percentage=0.025):\n",
    "    \"\"\"\n",
    "    Creates masks that are more likely to cover facial features.\n",
    "    \"\"\"\n",
    "    batch_size, _, height, width = image.shape\n",
    "    mask = torch.ones_like(image)\n",
    "\n",
    "    # Face regions typically in center 60% of image\n",
    "    center_bias = 0.3  # 30% border on each side\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        mask_h = int(np.sqrt(height * width * mask_percentage))\n",
    "        mask_w = mask_h\n",
    "\n",
    "        # Bias towards center\n",
    "        top_min = int(height * center_bias)\n",
    "        top_max = int(height * (1 - center_bias)) - mask_h\n",
    "        left_min = int(width * center_bias)\n",
    "        left_max = int(width * (1 - center_bias)) - mask_w\n",
    "\n",
    "        top = np.random.randint(top_min, max(top_min + 1, top_max))\n",
    "        left = np.random.randint(left_min, max(left_min + 1, left_max))\n",
    "\n",
    "        mask[i, :, top:top+mask_h, left:left+mask_w] = 0\n",
    "\n",
    "    masked_image = image * mask\n",
    "    return masked_image, mask"
   ],
   "metadata": {
    "id": "mgWR39z7wnAI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754624683020,
     "user_tz": -120,
     "elapsed": 2,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\" A simple self-attention layer \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.query = nn.Conv2d(channels, channels // 8, 1)\n",
    "        self.key   = nn.Conv2d(channels, channels // 8, 1)\n",
    "        self.value = nn.Conv2d(channels, channels, 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, C, width, height = x.size()\n",
    "        q = self.query(x).view(batch_size, -1, width * height).permute(0, 2, 1)\n",
    "        k = self.key(x).view(batch_size, -1, width * height)\n",
    "        v = self.value(x).view(batch_size, -1, width * height)\n",
    "\n",
    "        attention_map = F.softmax(torch.bmm(q, k), dim=-1)\n",
    "\n",
    "        out = torch.bmm(v, attention_map.permute(0, 2, 1))\n",
    "        out = out.view(batch_size, C, width, height)\n",
    "\n",
    "        return self.gamma * out + x # Add skip connection\n",
    "\n",
    "class UpsampleBlock(nn.Module):\n",
    "    \"\"\"An upsampling block using Conv2d and PixelShuffle.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # The Conv2d layer produces 4x the channels for a 2x upscale\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels * 4, kernel_size=3, padding=1)\n",
    "        self.pixel_shuffle = nn.PixelShuffle(2) # Rearranges channels to upscale by 2x\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.pixel_shuffle(self.conv(x)))\n",
    "\n",
    "class GatedConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    A Gated Convolutional Layer.\n",
    "    It learns a dynamic feature mask for each channel at every location.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1):\n",
    "        super().__init__()\n",
    "        # Convolution for the features\n",
    "        self.conv_feature = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size, stride, padding, dilation\n",
    "        )\n",
    "        # Convolution for the gating mechanism\n",
    "        self.conv_gate = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size, stride, padding, dilation\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get the features and the gate\n",
    "        features = self.conv_feature(x)\n",
    "        gate = torch.sigmoid(self.conv_gate(x)) # Gate values are between 0 and 1\n",
    "\n",
    "        # Element-wise multiplication to apply the learned gate\n",
    "        return features * gate\n",
    "\n",
    "class GatedResidualBlock(nn.Module):\n",
    "    \"\"\"A Residual Block that uses Gated Convolutions.\"\"\"\n",
    "    def __init__(self, channels, dilation=1):\n",
    "        super().__init__()\n",
    "        padding = dilation\n",
    "\n",
    "        # Replace nn.Conv2d with GatedConv2d\n",
    "        self.conv1 = GatedConv2d(channels, channels, kernel_size=3, padding=padding, dilation=dilation)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = GatedConv2d(channels, channels, kernel_size=3, padding=padding, dilation=dilation)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = out + residual  # Residual connection\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class UNetSR(nn.Module):\n",
    "    \"\"\"\n",
    "    A U-Net architecture with corrected channel dimensions for the decoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=4, out_channels=3, num_channels=64):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Initial Convolution ---\n",
    "        self.init_conv = nn.Conv2d(in_channels, num_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        # --- Encoder Path ---\n",
    "        self.enc1 = GatedResidualBlock(num_channels, dilation=1)\n",
    "        self.enc2 = GatedResidualBlock(num_channels, dilation=1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # --- Bottleneck with Dilation and Attention ---\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            GatedResidualBlock(num_channels, dilation=2),\n",
    "            # SelfAttention(num_channels), # Add attention layer\n",
    "            GatedResidualBlock(num_channels, dilation=4)\n",
    "        )\n",
    "\n",
    "        # --- Decoder Path ---\n",
    "        self.upconv2 = UpsampleBlock(num_channels, num_channels)\n",
    "        # Input channels = upsampled (64) + skip connection from e2 (64) = 128\n",
    "        self.dec2 = GatedResidualBlock(num_channels * 2, dilation=1)\n",
    "\n",
    "        self.upconv1 = UpsampleBlock(num_channels * 2, num_channels)\n",
    "        # Input channels = upsampled (64) + skip connection from e1 (64) = 128\n",
    "        self.dec1 = GatedResidualBlock(num_channels * 2, dilation=1)\n",
    "\n",
    "        # --- Final Output Layer ---\n",
    "        # The input to this layer comes from dec1, which outputs 128 channels.\n",
    "        self.out_conv = nn.Conv2d(num_channels * 2, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial feature extraction\n",
    "        x0 = self.init_conv(x)\n",
    "\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x0)\n",
    "        p1 = self.pool(e1)\n",
    "\n",
    "        e2 = self.enc2(p1)\n",
    "        p2 = self.pool(e2)\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(p2)\n",
    "\n",
    "        # Decoder with Skip Connections\n",
    "        d2 = self.upconv2(b)\n",
    "        d2 = torch.cat([d2, e2], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "\n",
    "        d1 = self.upconv1(d2)\n",
    "        d1 = torch.cat([d1, e1], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "\n",
    "        # Final Output\n",
    "        out = self.out_conv(d1)\n",
    "\n",
    "        return torch.tanh(out)"
   ],
   "metadata": {
    "id": "2i7V7OuMwo-y",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754624683055,
     "user_tz": -120,
     "elapsed": 32,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# --- 1. Create the main dataset ---\n",
    "# This should use the LARGER number of images you intend to work with.\n",
    "# Let's assume NUM_IMAGES_TO_USE_CNN is the total pool of images.\n",
    "print(\"Creating the main dataset...\")\n",
    "full_dataset = FFHQDataset(\n",
    "    img_dir=DATASET_PATH,\n",
    "    transform=transform,\n",
    "    num_images=NUM_IMAGES_TO_USE_CNN # Use the total number of images available for the experiment\n",
    ")\n",
    "print(f\"âœ… Main dataset created with {len(full_dataset)} images.\")\n",
    "\n",
    "# --- 2. Split the dataset into Training, Validation, and Test sets ---\n",
    "print(\"\\nSplitting data into training, validation, and test sets...\")\n",
    "dataset_size = len(full_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "np.random.seed(42) # for reproducibility\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Define split points for an 80/10/10 split\n",
    "train_split = int(np.floor(0.8 * dataset_size))\n",
    "val_split = int(np.floor(0.9 * dataset_size))\n",
    "\n",
    "# Create indices for each set\n",
    "train_indices = indices[:train_split]\n",
    "val_indices = indices[train_split:val_split]\n",
    "test_indices = indices[val_split:]\n",
    "\n",
    "# Create PyTorch Subsets\n",
    "train_data = Subset(full_dataset, train_indices)\n",
    "val_data = Subset(full_dataset, val_indices)\n",
    "test_data = Subset(full_dataset, test_indices)\n",
    "\n",
    "print(f\"âœ… Training set size: {len(train_data)}\")\n",
    "print(f\"âœ… Validation set size: {len(val_data)}\")\n",
    "print(f\"âœ… Test set size: {len(test_data)}\")\n",
    "\n",
    "# --- 3. Create the Diffusion Model's training subset ---\n",
    "# This should be a subset of the TRAINING data.\n",
    "print(\"\\nCreating a subset of the training data for the GAN model...\")\n",
    "gan_indices = train_indices[:NUM_IMAGES_TO_USE_GAN] # Take from the start of shuffled train indices\n",
    "gan_data = Subset(full_dataset, gan_indices)\n",
    "print(f\"âœ… Diffusion training set size: {len(gan_data)}\")\n",
    "\n",
    "\n",
    "# --- 4. Create DataLoaders for each set ---\n",
    "print(\"\\nCreating DataLoaders...\")\n",
    "# The main CNN will now train on the 'train_data' subset\n",
    "cnn_dataloader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "# New DataLoader for validation\n",
    "val_dataloader = DataLoader(\n",
    "    val_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False, # No need to shuffle validation data\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "# New DataLoader for testing\n",
    "test_dataloader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False, # No need to shuffle test data\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "# GAN DataLoader uses its own subset of the training data\n",
    "gan_dataloader = DataLoader(\n",
    "    gan_data,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "print(\"âœ… All DataLoaders created.\")\n",
    "\n",
    "# --- 5. Initialize models and optimizers (unchanged) ---\n",
    "print(\"\\nInitializing models and optimizers...\")\n",
    "cnn_model = UNetSR().to(DEVICE)\n",
    "\n",
    "# Compile the models for a speed boost\n",
    "cnn_model = torch.compile(cnn_model)\n",
    "\n",
    "# Initialize optimizers\n",
    "optimizer_cnn = optim.Adam(cnn_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"\\nSetup complete. Ready for CNN training!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Nf727i8wtoS",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754624685112,
     "user_tz": -120,
     "elapsed": 2053,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    },
    "outputId": "91b6864f-3dee-4625-b1dc-82de0d6ebff5"
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Creating the main dataset...\n",
      "âœ… Main dataset created with 10000 images.\n",
      "\n",
      "Splitting data into training, validation, and test sets...\n",
      "âœ… Training set size: 8000\n",
      "âœ… Validation set size: 1000\n",
      "âœ… Test set size: 1000\n",
      "\n",
      "Creating a subset of the training data for the GAN model...\n",
      "âœ… Diffusion training set size: 1000\n",
      "\n",
      "Creating DataLoaders...\n",
      "âœ… All DataLoaders created.\n",
      "\n",
      "Initializing models and optimizers...\n",
      "\n",
      "Setup complete. Ready for CNN training!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Stage 1: Pre-train the CNN ---\n",
    "print(\"--- Stage 1: Pre-training CNN ---\")\n",
    "\n",
    "ACCUMULATION_STEPS = 4\n",
    "WARMUP_EPOCHS = 5\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Initialize the DWT transform\n",
    "dwt = DWTForward(J=1, mode='zero', wave='haar').to(DEVICE)\n",
    "alpha = 0.1  # Weight for FFT Loss\n",
    "beta = 0.1   # Weight for DWT Loss\n",
    "\n",
    "# Add scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_cnn, mode='min', factor=0.5, patience=10\n",
    ")\n",
    "\n",
    "# Initialize a Gradient Scaler for the 'cuda' device\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "for epoch in tqdm(range(NUM_EPOCHS_CNN), desc=\"CNN Epochs\"):\n",
    "    cnn_model.train()\n",
    "    epoch_losses = []\n",
    "\n",
    "    for i, batch in enumerate(cnn_dataloader):\n",
    "        original_image = batch.to(DEVICE)\n",
    "        masked_image, mask = create_mask(original_image)\n",
    "        mask = mask.to(DEVICE)\n",
    "\n",
    "        # --- Part 1: Run the main CNN in Mixed Precision for speed ---\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            cnn_input = torch.cat((masked_image, mask[:, 0:1, :, :]), dim=1)\n",
    "            coarse_inpainting = cnn_model(cnn_input)\n",
    "\n",
    "            # --- Spatial Domain Loss (can stay in autocast) ---\n",
    "            hole_mask = 1 - mask\n",
    "            valid_loss = nn.functional.l1_loss(\n",
    "                coarse_inpainting * mask,\n",
    "                original_image * mask,\n",
    "                reduction='sum'\n",
    "            ) / mask.sum().clamp(min=1)\n",
    "\n",
    "            hole_loss = 10 * nn.functional.l1_loss(\n",
    "                coarse_inpainting * hole_mask,\n",
    "                original_image * hole_mask,\n",
    "                reduction='sum'\n",
    "            ) / hole_mask.sum().clamp(min=1)\n",
    "\n",
    "            l1_loss = valid_loss + hole_loss\n",
    "\n",
    "        # --- Part 2: Calculate sensitive losses in Full Precision (float32) ---\n",
    "        # Explicitly cast the output to float32 before FFT and DWT\n",
    "        coarse_inpainting_fp32 = coarse_inpainting.float()\n",
    "\n",
    "        if epoch < WARMUP_EPOCHS:\n",
    "            loss = l1_loss\n",
    "        else:\n",
    "            # --- FFT Loss (in float32) ---\n",
    "            fft_pred = torch.fft.fft2(coarse_inpainting_fp32, dim=(-2, -1))\n",
    "            fft_gt = torch.fft.fft2(original_image, dim=(-2, -1))\n",
    "            fft_loss = nn.functional.mse_loss(torch.abs(fft_pred), torch.abs(fft_gt))\n",
    "\n",
    "            # --- DWT Loss (in float32) ---\n",
    "            _, Yh_pred = dwt(coarse_inpainting_fp32)\n",
    "            _, Yh_gt = dwt(original_image)\n",
    "            dwt_loss = nn.functional.l1_loss(Yh_pred[0], Yh_gt[0])\n",
    "\n",
    "            # Combine all losses. PyTorch handles the type promotion automatically.\n",
    "            loss = l1_loss + alpha * fft_loss + beta * dwt_loss\n",
    "\n",
    "        # --- Backpropagation (unchanged) ---\n",
    "        loss = loss / ACCUMULATION_STEPS\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (i + 1) % ACCUMULATION_STEPS == 0:\n",
    "            scaler.step(optimizer_cnn)\n",
    "            scaler.update()\n",
    "            optimizer_cnn.zero_grad()\n",
    "\n",
    "        epoch_losses.append(loss.item() * ACCUMULATION_STEPS)\n",
    "\n",
    "    avg_train_loss = np.mean(epoch_losses)\n",
    "    print(f\"\\nEpoch {epoch}: Train Loss = {avg_train_loss:.4f}\")\n",
    "\n",
    "print(\"âœ… CNN Pre-training Complete.\")\n",
    "torch.save(cnn_model.state_dict(), 'final_cnn_model.pth')"
   ],
   "metadata": {
    "id": "y-JQlqwDwxrk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# --- CNN Evaluation with Comprehensive Metrics ---\n",
    "print(\"--- Evaluating CNN Model ---\")\n",
    "\n",
    "# Initialize the correct model\n",
    "cnn_model = UNetSR(in_channels=4, out_channels=3)\n",
    "\n",
    "# Load weights\n",
    "state_dict = torch.load('/content/drive/MyDrive/ColabNotebooks/Vision/inpainting/final_cnn_model.pth', map_location=DEVICE)\n",
    "if any(k.startswith('_orig_mod.') for k in state_dict.keys()):\n",
    "    state_dict = {k.replace('_orig_mod.', ''): v for k, v in state_dict.items()}\n",
    "cnn_model.load_state_dict(state_dict)\n",
    "\n",
    "cnn_model.to(DEVICE)\n",
    "cnn_model.eval()\n",
    "\n",
    "# --- Initialize All Metrics ---\n",
    "# 1. Basic metrics\n",
    "psnr_metric = torchmetrics.PeakSignalNoiseRatio(data_range=1.0).to(DEVICE)\n",
    "ssim_metric = torchmetrics.StructuralSimilarityIndexMeasure(data_range=1.0).to(DEVICE)\n",
    "\n",
    "# 2. Perceptual metric (used in ResDiff paper)\n",
    "lpips_metric = lpips.LPIPS(net='alex').to(DEVICE)  # Lower is better\n",
    "\n",
    "# 3. Feature-based metric\n",
    "fid_metric = FrechetInceptionDistance(feature=2048).to(DEVICE)\n",
    "\n",
    "# Get evaluation batch\n",
    "eval_dataloader = val_dataloader\n",
    "\n",
    "# Collect metrics across multiple batches\n",
    "all_psnr = []\n",
    "all_ssim = []\n",
    "all_lpips = []\n",
    "all_mse = []\n",
    "all_mae = []\n",
    "all_masked_mse = []\n",
    "all_masked_mae = []\n",
    "\n",
    "print(\"Evaluating on multiple batches...\")\n",
    "for batch_idx, batch in enumerate(eval_dataloader):\n",
    "    if batch_idx >= 10:  # Evaluate on 10 batches\n",
    "        break\n",
    "\n",
    "    original_images = batch.to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        masked_images, masks = create_mask(original_images)\n",
    "        masks = masks.to(DEVICE)\n",
    "\n",
    "        cnn_input = torch.cat((masked_images, masks[:, 0:1, :, :]), dim=1)\n",
    "        cnn_reconstructions = cnn_model(cnn_input)\n",
    "\n",
    "    # Denormalize from [-1, 1] to [0, 1]\n",
    "    original_denorm = (original_images + 1) / 2\n",
    "    recon_denorm = (cnn_reconstructions + 1) / 2\n",
    "\n",
    "    # 1. PSNR and SSIM\n",
    "    batch_psnr = psnr_metric(recon_denorm, original_denorm)\n",
    "    batch_ssim = ssim_metric(recon_denorm, original_denorm)\n",
    "    all_psnr.append(batch_psnr.item())\n",
    "    all_ssim.append(batch_ssim.item())\n",
    "\n",
    "    # 2. LPIPS (Perceptual loss)\n",
    "    batch_lpips = lpips_metric(original_denorm * 2 - 1, recon_denorm * 2 - 1)  # LPIPS expects [-1, 1]\n",
    "    all_lpips.append(batch_lpips.mean().item())\n",
    "\n",
    "    # 3. MSE and MAE\n",
    "    mse = torch.nn.functional.mse_loss(recon_denorm, original_denorm)\n",
    "    mae = torch.nn.functional.l1_loss(recon_denorm, original_denorm)\n",
    "    all_mse.append(mse.item())\n",
    "    all_mae.append(mae.item())\n",
    "\n",
    "    # 4. Masked region metrics\n",
    "    mask_binary = (masks[:, 0:1, :, :] == 0).float()\n",
    "    if mask_binary.sum() > 0:\n",
    "        masked_mse = ((recon_denorm - original_denorm) ** 2 * mask_binary).sum() / mask_binary.sum()\n",
    "        masked_mae = (torch.abs(recon_denorm - original_denorm) * mask_binary).sum() / mask_binary.sum()\n",
    "        all_masked_mse.append(masked_mse.item())\n",
    "        all_masked_mae.append(masked_mae.item())\n",
    "\n",
    "    # Update FID metric\n",
    "    # Convert to uint8 format expected by FID\n",
    "    real_images_uint8 = (original_denorm * 255).to(torch.uint8)\n",
    "    fake_images_uint8 = (recon_denorm * 255).to(torch.uint8)\n",
    "    fid_metric.update(real_images_uint8, real=True)\n",
    "    fid_metric.update(fake_images_uint8, real=False)\n",
    "\n",
    "# Calculate FID\n",
    "fid_score = fid_metric.compute()\n",
    "\n",
    "# --- Print Comprehensive Results ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CNN INPAINTING EVALUATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nğŸ“Š RECONSTRUCTION METRICS:\")\n",
    "print(f\"  PSNR:  {np.mean(all_psnr):.2f} Â± {np.std(all_psnr):.2f} dB\")\n",
    "print(f\"  SSIM:  {np.mean(all_ssim):.4f} Â± {np.std(all_ssim):.4f}\")\n",
    "print(f\"  MSE:   {np.mean(all_mse):.4f} Â± {np.std(all_mse):.4f}\")\n",
    "print(f\"  MAE:   {np.mean(all_mae):.4f} Â± {np.std(all_mae):.4f}\")\n",
    "\n",
    "print(\"\\nğŸ¨ PERCEPTUAL METRICS:\")\n",
    "print(f\"  LPIPS: {np.mean(all_lpips):.4f} Â± {np.std(all_lpips):.4f} (lower is better)\")\n",
    "print(f\"  FID:   {fid_score:.2f} (lower is better)\")\n",
    "\n",
    "print(\"\\nğŸ¯ MASKED REGION METRICS:\")\n",
    "print(f\"  Masked MSE: {np.mean(all_masked_mse):.4f} Â± {np.std(all_masked_mse):.4f}\")\n",
    "print(f\"  Masked MAE: {np.mean(all_masked_mae):.4f} Â± {np.std(all_masked_mae):.4f}\")\n",
    "\n",
    "# --- Frequency Domain Analysis ---\n",
    "print(\"\\nğŸ“ˆ FREQUENCY DOMAIN ANALYSIS:\")\n",
    "with torch.no_grad():\n",
    "    # Get one batch for frequency analysis\n",
    "    test_batch = next(iter(eval_dataloader)).to(DEVICE)\n",
    "    masked_test, masks_test = create_mask(test_batch)\n",
    "    masks_test = masks_test.to(DEVICE)\n",
    "\n",
    "    cnn_input_test = torch.cat((masked_test, masks_test[:, 0:1, :, :]), dim=1)\n",
    "    recon_test = cnn_model(cnn_input_test)\n",
    "\n",
    "    # FFT Analysis\n",
    "    fft_original = torch.fft.fft2(test_batch, dim=(-2, -1))\n",
    "    fft_recon = torch.fft.fft2(recon_test, dim=(-2, -1))\n",
    "\n",
    "    # Compare frequency magnitudes\n",
    "    mag_original = torch.abs(fft_original).mean()\n",
    "    mag_recon = torch.abs(fft_recon).mean()\n",
    "\n",
    "    # High frequency analysis (outer regions of FFT)\n",
    "    h, w = test_batch.shape[-2:]\n",
    "    mask_high_freq = torch.ones_like(test_batch)\n",
    "    mask_high_freq[:, :, h//4:3*h//4, w//4:3*w//4] = 0\n",
    "\n",
    "    high_freq_original = (torch.abs(fft_original) * mask_high_freq).mean()\n",
    "    high_freq_recon = (torch.abs(fft_recon) * mask_high_freq).mean()\n",
    "\n",
    "    print(f\"  Average Frequency Magnitude Ratio: {(mag_recon/mag_original):.3f}\")\n",
    "    print(f\"  High-Frequency Recovery Ratio: {(high_freq_recon/high_freq_original):.3f}\")\n",
    "\n",
    "# --- Visualization with Difference Maps ---\n",
    "num_to_show = 4\n",
    "fig, axes = plt.subplots(num_to_show, 5, figsize=(20, num_to_show * 4))\n",
    "fig.suptitle('CNN Inpainting Evaluation', fontsize=16)\n",
    "\n",
    "# Get samples for visualization\n",
    "vis_batch = next(iter(eval_dataloader)).to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    vis_masked, vis_masks = create_mask(vis_batch[:num_to_show])\n",
    "    vis_masks = vis_masks.to(DEVICE)\n",
    "    vis_input = torch.cat((vis_masked, vis_masks[:, 0:1, :, :]), dim=1)\n",
    "    vis_recon = cnn_model(vis_input)\n",
    "\n",
    "for i in range(num_to_show):\n",
    "    # Original\n",
    "    axes[i, 0].imshow(tensor_to_image(vis_batch[i]))\n",
    "    axes[i, 0].set_title(\"Original\")\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    # Masked Input\n",
    "    axes[i, 1].imshow(tensor_to_image(vis_masked[i]))\n",
    "    axes[i, 1].set_title(\"Masked Input\")\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "    # CNN Reconstruction\n",
    "    axes[i, 2].imshow(tensor_to_image(vis_recon[i]))\n",
    "    axes[i, 2].set_title(f\"CNN Output\")\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "    # Difference Map\n",
    "    diff = torch.abs(vis_batch[i] - vis_recon[i]).mean(dim=0)\n",
    "    im = axes[i, 3].imshow(diff.cpu().numpy(), cmap='hot')\n",
    "    axes[i, 3].set_title(\"Absolute Difference\")\n",
    "    axes[i, 3].axis('off')\n",
    "    plt.colorbar(im, ax=axes[i, 3], fraction=0.046)\n",
    "\n",
    "    # Masked Region Only\n",
    "    mask_vis = vis_masks[i, 0].cpu().numpy()\n",
    "    masked_recon = tensor_to_image(vis_recon[i])\n",
    "    masked_recon[mask_vis > 0.5] = 1  # Highlight unmasked regions\n",
    "    axes[i, 4].imshow(masked_recon)\n",
    "    axes[i, 4].set_title(\"Inpainted Region\")\n",
    "    axes[i, 4].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Save Results Summary ---\n",
    "results_summary = f\"\"\"\n",
    "CNN Inpainting Evaluation Results\n",
    "=================================\n",
    "PSNR: {np.mean(all_psnr):.2f} Â± {np.std(all_psnr):.2f} dB\n",
    "SSIM: {np.mean(all_ssim):.4f} Â± {np.std(all_ssim):.4f}\n",
    "LPIPS: {np.mean(all_lpips):.4f} Â± {np.std(all_lpips):.4f}\n",
    "FID: {fid_score:.2f}\n",
    "Masked MSE: {np.mean(all_masked_mse):.4f}\n",
    "Masked MAE: {np.mean(all_masked_mae):.4f}\n",
    "\"\"\"\n",
    "\n",
    "with open('cnn_evaluation_results.txt', 'w') as f:\n",
    "    f.write(results_summary)\n",
    "\n",
    "print(\"\\nâœ… Evaluation complete! Results saved to 'cnn_evaluation_results.txt'\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "14sv6tO6Svsc1xmrJFIt77DSe26pv7fh6"
    },
    "id": "agAHGTe5w2D0",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754624781673,
     "user_tz": -120,
     "elapsed": 7296,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    },
    "outputId": "754c25dd-0389-4d95-b0f8-143c989b2574"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Output hidden; open in https://colab.research.google.com to view."
     },
     "metadata": {}
    }
   ]
  }
 ]
}
