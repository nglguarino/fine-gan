{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100",
   "authorship_tag": "ABX9TyMl2ZI7TYglbxC2yrdTqwrX"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RMMlqkYC0NMw",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754625537317,
     "user_tz": -120,
     "elapsed": 20975,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    },
    "outputId": "c77a740d-92d3-495a-9b9c-870f5d56f6dd"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Unzip the dataset ---\n",
    "# IMPORTANT: Update this path to match the location of your zip file in Drive.\n",
    "zip_path = '/content/drive/MyDrive/ColabNotebooks/Vision/inpainting/ffhq256_10ksubset.zip'\n",
    "\n",
    "# The destination folder in the local Colab environment\n",
    "destination_path = '/content'\n",
    "\n",
    "print(\"Unzipping dataset...\")\n",
    "# The -q flag makes the output cleaner (quiet mode)\n",
    "!unzip -q {zip_path} -d {destination_path}\n",
    "\n",
    "print(f\"✅ Dataset unzipped to {destination_path}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c_rH_lvB0Vwl",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754625559500,
     "user_tz": -120,
     "elapsed": 22177,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    },
    "outputId": "8d800701-bc06-4039-b936-6f4241f1ded0"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unzipping dataset...\n",
      "✅ Dataset unzipped to /content\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Install\n",
    "!pip -q install pytorch_wavelets torchmetrics lpips torch-fidelity pytorch-fid"
   ],
   "metadata": {
    "id": "YEDQAJB00Yzr",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754626519704,
     "user_tz": -120,
     "elapsed": 2618,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    }
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch._dynamo\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.nn.utils import spectral_norm\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm.autonotebook import tqdm\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchmetrics\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "import lpips\n",
    "\n",
    "import torch.fft\n",
    "from pytorch_wavelets import DWTForward\n",
    "from einops import rearrange\n",
    "\n",
    "import scipy.stats as stats\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from pytorch_fid import fid_score\n",
    "\n",
    "# Enable TensorFloat32\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# --- Configuration & Hyperparameters ---\n",
    "\n",
    "# Set the path to your image folder in Google Drive\n",
    "# IMPORTANT: Update this path to match where you saved your dataset.\n",
    "DATASET_PATH = '/content/ffhq_subset_10k'\n",
    "NUM_IMAGES_TO_USE_CNN = 10000\n",
    "NUM_IMAGES_TO_USE_GAN = 1000\n",
    "\n",
    "# Training settings\n",
    "NUM_EPOCHS_CNN = 40\n",
    "NUM_EPOCHS_GAN = 200\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# Set the device (use GPU if available)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y9lOjwd_0bEa",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754626523487,
     "user_tz": -120,
     "elapsed": 44,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    },
    "outputId": "ee8de7db-26e2-4eb7-b554-435f2b836391"
   },
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class FFHQDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset for loading FFHQ images.\"\"\"\n",
    "    def __init__(self, img_dir, transform=None, num_images=None): # Add num_images parameter\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            num_images (int, optional): Number of images to use. If None, use all images.\n",
    "        \"\"\"\n",
    "        # Find all files with .png or .jpg extension\n",
    "        self.img_paths = glob.glob(os.path.join(img_dir, '*.png'))\n",
    "        self.img_paths.extend(glob.glob(os.path.join(img_dir, '*.jpg')))\n",
    "        self.transform = transform\n",
    "\n",
    "        if num_images:\n",
    "            # If a number is specified, shuffle all paths and take a random subset\n",
    "            random.shuffle(self.img_paths)\n",
    "            self.img_paths = self.img_paths[:num_images]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "# Your transform definition remains the same\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ],
   "metadata": {
    "id": "_8c97qH00lzs",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754625659478,
     "user_tz": -120,
     "elapsed": 23,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def tensor_to_image(tensor):\n",
    "    \"\"\"\n",
    "    Converts a PyTorch tensor to a displayable NumPy image.\n",
    "    It denormalizes, moves to CPU, and changes dimension order.\n",
    "    \"\"\"\n",
    "    # Denormalize the image from [-1, 1] to [0, 1]\n",
    "    image = tensor * 0.5 + 0.5\n",
    "    # Move tensor to CPU and convert to NumPy array\n",
    "    image = image.cpu().numpy()\n",
    "    # Transpose dimensions from (C, H, W) to (H, W, C) for plotting\n",
    "    image = image.transpose(1, 2, 0)\n",
    "    # Clip values to be in the valid [0, 1] range for images\n",
    "    image = np.clip(image, 0, 1)\n",
    "    return image"
   ],
   "metadata": {
    "id": "rmdlduRN0oos",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754625659523,
     "user_tz": -120,
     "elapsed": 3,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def create_mask(image, mask_percentage=0.025):\n",
    "    \"\"\"\n",
    "    Creates masks that are more likely to cover facial features.\n",
    "    \"\"\"\n",
    "    batch_size, _, height, width = image.shape\n",
    "    mask = torch.ones_like(image)\n",
    "\n",
    "    # Face regions typically in center 60% of image\n",
    "    center_bias = 0.3  # 30% border on each side\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        mask_h = int(np.sqrt(height * width * mask_percentage))\n",
    "        mask_w = mask_h\n",
    "\n",
    "        # Bias towards center\n",
    "        top_min = int(height * center_bias)\n",
    "        top_max = int(height * (1 - center_bias)) - mask_h\n",
    "        left_min = int(width * center_bias)\n",
    "        left_max = int(width * (1 - center_bias)) - mask_w\n",
    "\n",
    "        top = np.random.randint(top_min, max(top_min + 1, top_max))\n",
    "        left = np.random.randint(left_min, max(left_min + 1, left_max))\n",
    "\n",
    "        mask[i, :, top:top+mask_h, left:left+mask_w] = 0\n",
    "\n",
    "    masked_image = image * mask\n",
    "    return masked_image, mask"
   ],
   "metadata": {
    "id": "Y5Jjq2iV0rI5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754625659566,
     "user_tz": -120,
     "elapsed": 6,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\" A simple self-attention layer \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.query = nn.Conv2d(channels, channels // 8, 1)\n",
    "        self.key   = nn.Conv2d(channels, channels // 8, 1)\n",
    "        self.value = nn.Conv2d(channels, channels, 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, C, width, height = x.size()\n",
    "        q = self.query(x).view(batch_size, -1, width * height).permute(0, 2, 1)\n",
    "        k = self.key(x).view(batch_size, -1, width * height)\n",
    "        v = self.value(x).view(batch_size, -1, width * height)\n",
    "\n",
    "        attention_map = F.softmax(torch.bmm(q, k), dim=-1)\n",
    "\n",
    "        out = torch.bmm(v, attention_map.permute(0, 2, 1))\n",
    "        out = out.view(batch_size, C, width, height)\n",
    "\n",
    "        return self.gamma * out + x # Add skip connection\n",
    "\n",
    "class UpsampleBlock(nn.Module):\n",
    "    \"\"\"An upsampling block using Conv2d and PixelShuffle.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # The Conv2d layer produces 4x the channels for a 2x upscale\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels * 4, kernel_size=3, padding=1)\n",
    "        self.pixel_shuffle = nn.PixelShuffle(2) # Rearranges channels to upscale by 2x\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.pixel_shuffle(self.conv(x)))\n",
    "\n",
    "class GatedConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    A Gated Convolutional Layer.\n",
    "    It learns a dynamic feature mask for each channel at every location.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1):\n",
    "        super().__init__()\n",
    "        # Convolution for the features\n",
    "        self.conv_feature = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size, stride, padding, dilation\n",
    "        )\n",
    "        # Convolution for the gating mechanism\n",
    "        self.conv_gate = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size, stride, padding, dilation\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get the features and the gate\n",
    "        features = self.conv_feature(x)\n",
    "        gate = torch.sigmoid(self.conv_gate(x)) # Gate values are between 0 and 1\n",
    "\n",
    "        # Element-wise multiplication to apply the learned gate\n",
    "        return features * gate\n",
    "\n",
    "class GatedResidualBlock(nn.Module):\n",
    "    \"\"\"A Residual Block that uses Gated Convolutions.\"\"\"\n",
    "    def __init__(self, channels, dilation=1):\n",
    "        super().__init__()\n",
    "        padding = dilation\n",
    "\n",
    "        # Replace nn.Conv2d with GatedConv2d\n",
    "        self.conv1 = GatedConv2d(channels, channels, kernel_size=3, padding=padding, dilation=dilation)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = GatedConv2d(channels, channels, kernel_size=3, padding=padding, dilation=dilation)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = out + residual  # Residual connection\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class UNetSR(nn.Module):\n",
    "    \"\"\"\n",
    "    A U-Net architecture with corrected channel dimensions for the decoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=4, out_channels=3, num_channels=64):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Initial Convolution ---\n",
    "        self.init_conv = nn.Conv2d(in_channels, num_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        # --- Encoder Path ---\n",
    "        self.enc1 = GatedResidualBlock(num_channels, dilation=1)\n",
    "        self.enc2 = GatedResidualBlock(num_channels, dilation=1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # --- Bottleneck with Dilation and Attention ---\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            GatedResidualBlock(num_channels, dilation=2),\n",
    "            # SelfAttention(num_channels), # Add attention layer\n",
    "            GatedResidualBlock(num_channels, dilation=4)\n",
    "        )\n",
    "\n",
    "        # --- Decoder Path ---\n",
    "        self.upconv2 = UpsampleBlock(num_channels, num_channels)\n",
    "        # Input channels = upsampled (64) + skip connection from e2 (64) = 128\n",
    "        self.dec2 = GatedResidualBlock(num_channels * 2, dilation=1)\n",
    "\n",
    "        self.upconv1 = UpsampleBlock(num_channels * 2, num_channels)\n",
    "        # Input channels = upsampled (64) + skip connection from e1 (64) = 128\n",
    "        self.dec1 = GatedResidualBlock(num_channels * 2, dilation=1)\n",
    "\n",
    "        # --- Final Output Layer ---\n",
    "        # The input to this layer comes from dec1, which outputs 128 channels.\n",
    "        self.out_conv = nn.Conv2d(num_channels * 2, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial feature extraction\n",
    "        x0 = self.init_conv(x)\n",
    "\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x0)\n",
    "        p1 = self.pool(e1)\n",
    "\n",
    "        e2 = self.enc2(p1)\n",
    "        p2 = self.pool(e2)\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(p2)\n",
    "\n",
    "        # Decoder with Skip Connections\n",
    "        d2 = self.upconv2(b)\n",
    "        d2 = torch.cat([d2, e2], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "\n",
    "        d1 = self.upconv1(d2)\n",
    "        d1 = torch.cat([d1, e1], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "\n",
    "        # Final Output\n",
    "        out = self.out_conv(d1)\n",
    "\n",
    "        return torch.tanh(out)"
   ],
   "metadata": {
    "id": "loNrcp9a1w6A",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754625679192,
     "user_tz": -120,
     "elapsed": 72,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# --- 1. Create the main dataset ---\n",
    "# This should use the LARGER number of images you intend to work with.\n",
    "# Let's assume NUM_IMAGES_TO_USE_CNN is the total pool of images.\n",
    "print(\"Creating the main dataset...\")\n",
    "full_dataset = FFHQDataset(\n",
    "    img_dir=DATASET_PATH,\n",
    "    transform=transform,\n",
    "    num_images=NUM_IMAGES_TO_USE_CNN # Use the total number of images available for the experiment\n",
    ")\n",
    "print(f\"✅ Main dataset created with {len(full_dataset)} images.\")\n",
    "\n",
    "# --- 2. Split the dataset into Training, Validation, and Test sets ---\n",
    "print(\"\\nSplitting data into training, validation, and test sets...\")\n",
    "dataset_size = len(full_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "np.random.seed(42) # for reproducibility\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Define split points for an 80/10/10 split\n",
    "train_split = int(np.floor(0.8 * dataset_size))\n",
    "val_split = int(np.floor(0.9 * dataset_size))\n",
    "\n",
    "# Create indices for each set\n",
    "train_indices = indices[:train_split]\n",
    "val_indices = indices[train_split:val_split]\n",
    "test_indices = indices[val_split:]\n",
    "\n",
    "# Create PyTorch Subsets\n",
    "train_data = Subset(full_dataset, train_indices)\n",
    "val_data = Subset(full_dataset, val_indices)\n",
    "test_data = Subset(full_dataset, test_indices)\n",
    "\n",
    "print(f\"✅ Training set size: {len(train_data)}\")\n",
    "print(f\"✅ Validation set size: {len(val_data)}\")\n",
    "print(f\"✅ Test set size: {len(test_data)}\")\n",
    "\n",
    "# --- 3. Create the Diffusion Model's training subset ---\n",
    "# This should be a subset of the TRAINING data.\n",
    "print(\"\\nCreating a subset of the training data for the GAN model...\")\n",
    "gan_indices = train_indices[:NUM_IMAGES_TO_USE_GAN] # Take from the start of shuffled train indices\n",
    "gan_data = Subset(full_dataset, gan_indices)\n",
    "print(f\"✅ Diffusion training set size: {len(gan_data)}\")\n",
    "\n",
    "\n",
    "# --- 4. Create DataLoaders for each set ---\n",
    "print(\"\\nCreating DataLoaders...\")\n",
    "# The main CNN will now train on the 'train_data' subset\n",
    "cnn_dataloader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "# New DataLoader for validation\n",
    "val_dataloader = DataLoader(\n",
    "    val_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False, # No need to shuffle validation data\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "# New DataLoader for testing\n",
    "test_dataloader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False, # No need to shuffle test data\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "# GAN DataLoader uses its own subset of the training data\n",
    "gan_dataloader = DataLoader(\n",
    "    gan_data,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "print(\"✅ All DataLoaders created.\")\n",
    "\n",
    "# --- 5. Initialize models and optimizers (unchanged) ---\n",
    "print(\"\\nInitializing models and optimizers...\")\n",
    "cnn_model = UNetSR().to(DEVICE)\n",
    "\n",
    "# Compile the models for a speed boost\n",
    "cnn_model = torch.compile(cnn_model)\n",
    "\n",
    "# Initialize optimizers\n",
    "optimizer_cnn = optim.Adam(cnn_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"\\nSetup complete. Ready for CNN training!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZC-hFsMn0dbA",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754625684215,
     "user_tz": -120,
     "elapsed": 2173,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    },
    "outputId": "cdedc490-1934-4a9a-c34d-13f5fef27629"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Creating the main dataset...\n",
      "✅ Main dataset created with 10000 images.\n",
      "\n",
      "Splitting data into training, validation, and test sets...\n",
      "✅ Training set size: 8000\n",
      "✅ Validation set size: 1000\n",
      "✅ Test set size: 1000\n",
      "\n",
      "Creating a subset of the training data for the GAN model...\n",
      "✅ Diffusion training set size: 1000\n",
      "\n",
      "Creating DataLoaders...\n",
      "✅ All DataLoaders created.\n",
      "\n",
      "Initializing models and optimizers...\n",
      "\n",
      "Setup complete. Ready for CNN training!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# 1. GENERATOR - Residual Refinement Network\n",
    "# ============================================\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block with spectral normalization\"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = spectral_norm(nn.Conv2d(channels, channels, 3, 1, 1))\n",
    "        self.conv2 = spectral_norm(nn.Conv2d(channels, channels, 3, 1, 1))\n",
    "        self.norm1 = nn.InstanceNorm2d(channels)\n",
    "        self.norm2 = nn.InstanceNorm2d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.leaky_relu(self.norm1(self.conv1(x)), 0.2)\n",
    "        x = self.norm2(self.conv2(x))\n",
    "        return x + residual\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"Self-attention layer for capturing long-range dependencies\"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.query = spectral_norm(nn.Conv2d(channels, channels // 8, 1))\n",
    "        self.key = spectral_norm(nn.Conv2d(channels, channels // 8, 1))\n",
    "        self.value = spectral_norm(nn.Conv2d(channels, channels, 1))\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "\n",
    "        q = self.query(x).view(b, -1, h * w).permute(0, 2, 1)\n",
    "        k = self.key(x).view(b, -1, h * w)\n",
    "        v = self.value(x).view(b, -1, h * w)\n",
    "\n",
    "        attention = F.softmax(torch.bmm(q, k), dim=-1)\n",
    "        out = torch.bmm(v, attention.permute(0, 2, 1))\n",
    "        out = out.view(b, c, h, w)\n",
    "\n",
    "        return self.gamma * out + x\n",
    "\n",
    "class RefinementGenerator(nn.Module):\n",
    "    \"\"\"Generator that refines CNN output\"\"\"\n",
    "    def __init__(self, in_channels=7, out_channels=3, base_channels=64):\n",
    "        super().__init__()\n",
    "\n",
    "        # Input: CNN output (3) + original masked (3) + mask (1) = 7 channels\n",
    "        self.encoder = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(in_channels, base_channels, 7, 1, 3)),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "\n",
    "        # Downsampling\n",
    "        self.down1 = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(base_channels, base_channels * 2, 4, 2, 1)),\n",
    "            nn.InstanceNorm2d(base_channels * 2),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "\n",
    "        self.down2 = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(base_channels * 2, base_channels * 4, 4, 2, 1)),\n",
    "            nn.InstanceNorm2d(base_channels * 4),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "\n",
    "        # Residual blocks with attention\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            ResidualBlock(base_channels * 4),\n",
    "            ResidualBlock(base_channels * 4),\n",
    "            SelfAttention(base_channels * 4),\n",
    "            ResidualBlock(base_channels * 4),\n",
    "            ResidualBlock(base_channels * 4),\n",
    "        )\n",
    "\n",
    "        # Upsampling\n",
    "        self.up1 = nn.Sequential(\n",
    "            spectral_norm(nn.ConvTranspose2d(base_channels * 4, base_channels * 2, 4, 2, 1)),\n",
    "            nn.InstanceNorm2d(base_channels * 2),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "\n",
    "        self.up2 = nn.Sequential(\n",
    "            spectral_norm(nn.ConvTranspose2d(base_channels * 2, base_channels, 4, 2, 1)),\n",
    "            nn.InstanceNorm2d(base_channels),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "\n",
    "        # Output residual\n",
    "        self.output = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(base_channels, out_channels, 7, 1, 3)),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        # Learnable residual weight\n",
    "        self.residual_weight = nn.Parameter(torch.tensor(0.01))\n",
    "\n",
    "    def forward(self, cnn_output, masked_image, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cnn_output: CNN's coarse prediction [B, 3, H, W]\n",
    "            masked_image: Original image with mask applied [B, 3, H, W]\n",
    "            mask: Binary mask [B, 1, H, W]\n",
    "        \"\"\"\n",
    "        # Concatenate inputs\n",
    "        x = torch.cat([cnn_output, masked_image, mask], dim=1)\n",
    "\n",
    "        # Encode\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Downsample\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "\n",
    "        # Process with residual blocks\n",
    "        x = self.res_blocks(d2)\n",
    "\n",
    "        # Upsample with skip connections\n",
    "        x = self.up1(x)\n",
    "        x = self.up2(x + d1)  # Skip connection\n",
    "\n",
    "        # Generate residual\n",
    "        residual = self.output(x)\n",
    "\n",
    "        # Add weighted residual to CNN output\n",
    "        refined = cnn_output + self.residual_weight * residual\n",
    "\n",
    "        # Ensure output is in [-1, 1]\n",
    "        return torch.tanh(refined)"
   ],
   "metadata": {
    "id": "mSdcxtHr0yPW",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754625686969,
     "user_tz": -120,
     "elapsed": 25,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    }
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# 2. DISCRIMINATOR - Multi-scale PatchGAN\n",
    "# ============================================\n",
    "\n",
    "class MultiscaleDiscriminator(nn.Module):\n",
    "    \"\"\"Multi-scale discriminator for better gradient flow\"\"\"\n",
    "    def __init__(self, in_channels=3, base_channels=64, num_scales=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_scales = num_scales\n",
    "        self.discriminators = nn.ModuleList()\n",
    "\n",
    "        for _ in range(num_scales):\n",
    "            self.discriminators.append(self._make_discriminator(in_channels, base_channels))\n",
    "\n",
    "        self.downsample = nn.AvgPool2d(2)\n",
    "\n",
    "    def _make_discriminator(self, in_channels, base_channels):\n",
    "        return nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(in_channels, base_channels, 4, 2, 1)),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "\n",
    "            spectral_norm(nn.Conv2d(base_channels, base_channels * 2, 4, 2, 1)),\n",
    "            nn.InstanceNorm2d(base_channels * 2),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "\n",
    "            spectral_norm(nn.Conv2d(base_channels * 2, base_channels * 4, 4, 2, 1)),\n",
    "            nn.InstanceNorm2d(base_channels * 4),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "\n",
    "            spectral_norm(nn.Conv2d(base_channels * 4, base_channels * 8, 4, 1, 1)),\n",
    "            nn.InstanceNorm2d(base_channels * 8),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "\n",
    "            spectral_norm(nn.Conv2d(base_channels * 8, 1, 4, 1, 1))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "\n",
    "        for i in range(self.num_scales):\n",
    "            outputs.append(self.discriminators[i](x))\n",
    "            if i < self.num_scales - 1:\n",
    "                x = self.downsample(x)\n",
    "\n",
    "        return outputs"
   ],
   "metadata": {
    "id": "egVHxjX2000q",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754625691345,
     "user_tz": -120,
     "elapsed": 23,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# 3. LOSS FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    \"\"\"VGG-based perceptual loss\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        import torchvision.models as models\n",
    "        from torchvision.models import VGG19_Weights\n",
    "\n",
    "        vgg = models.vgg19(weights=VGG19_Weights.IMAGENET1K_V1).features\n",
    "\n",
    "        # Extract specific layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            vgg[:4],   # relu1_2\n",
    "            vgg[4:9],  # relu2_2\n",
    "            vgg[9:18], # relu3_4\n",
    "        ])\n",
    "\n",
    "        # Freeze VGG\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Normalization\n",
    "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
    "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # Normalize from [-1, 1] to [0, 1]\n",
    "        pred = (pred + 1) / 2\n",
    "        target = (target + 1) / 2\n",
    "\n",
    "        # VGG normalization\n",
    "        pred = (pred - self.mean) / self.std\n",
    "        target = (target - self.mean) / self.std\n",
    "\n",
    "        loss = 0\n",
    "        x_pred, x_target = pred, target\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x_pred = layer(x_pred)\n",
    "            x_target = layer(x_target)\n",
    "            loss += F.l1_loss(x_pred, x_target)\n",
    "\n",
    "        return loss\n",
    "\n",
    "def hinge_loss_d(real_pred, fake_pred):\n",
    "    \"\"\"Hinge loss for discriminator\"\"\"\n",
    "    loss = 0\n",
    "    for real_p, fake_p in zip(real_pred, fake_pred):\n",
    "        loss += torch.mean(F.relu(1 - real_p)) + torch.mean(F.relu(1 + fake_p))\n",
    "    return loss / len(real_pred)\n",
    "\n",
    "def hinge_loss_g(fake_pred):\n",
    "    \"\"\"Hinge loss for generator\"\"\"\n",
    "    loss = 0\n",
    "    for fake_p in fake_pred:\n",
    "        loss += -torch.mean(fake_p)\n",
    "    return loss / len(fake_pred)"
   ],
   "metadata": {
    "id": "SpWzgGcC03UW",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754625693657,
     "user_tz": -120,
     "elapsed": 22,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    }
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# 4. TRAINING FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def train_gan(generator, discriminator, cnn_model, train_loader, val_loader,\n",
    "              num_epochs=80, device='cuda'):\n",
    "    \"\"\"Train the GAN for refinement\"\"\"\n",
    "\n",
    "    # Optimizers\n",
    "    g_optimizer = optim.Adam(generator.parameters(), lr=1e-4, betas=(0.0, 0.999))\n",
    "    d_optimizer = optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0.0, 0.999))\n",
    "\n",
    "    # Add GradScalers for mixed precision\n",
    "    g_scaler = torch.amp.GradScaler('cuda')\n",
    "    d_scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "    # Loss functions\n",
    "    l1_loss = nn.L1Loss()\n",
    "    perceptual_loss = PerceptualLoss().to(device)\n",
    "\n",
    "    # Training history\n",
    "    history = {'g_loss': [], 'd_loss': [], 'val_psnr': []}\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "\n",
    "        epoch_g_loss = 0\n",
    "        epoch_d_loss = 0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            real_images = batch.to(device)\n",
    "            batch_size = real_images.size(0)\n",
    "\n",
    "            # Create masks and get CNN predictions\n",
    "            with torch.no_grad():\n",
    "                masked_images, masks = create_mask(real_images, mask_percentage=0.025)\n",
    "                masks = masks.to(device)\n",
    "\n",
    "                cnn_input = torch.cat([masked_images, masks[:, 0:1]], dim=1)\n",
    "                coarse_output = cnn_model(cnn_input)\n",
    "\n",
    "            # ==================\n",
    "            # Train Discriminator\n",
    "            # ==================\n",
    "            d_optimizer.zero_grad()\n",
    "\n",
    "            # Wrap forward passes in autocast\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                # Generate refined images\n",
    "                refined_images = generator(coarse_output, masked_images, masks[:, 0:1])\n",
    "\n",
    "                # Discriminator predictions\n",
    "                real_pred = discriminator(real_images)\n",
    "                fake_pred = discriminator(refined_images.detach())\n",
    "\n",
    "                # Hinge loss\n",
    "                d_loss = hinge_loss_d(real_pred, fake_pred)\n",
    "\n",
    "            # Use scaler for backward pass\n",
    "            d_scaler.scale(d_loss).backward()\n",
    "            d_scaler.step(d_optimizer)\n",
    "            d_scaler.update()\n",
    "\n",
    "            # ==================\n",
    "            # Train Generator\n",
    "            # ==================\n",
    "            # Train generator every 1 discriminator steps\n",
    "            # Modified loss to heavily prioritize hole improvement\n",
    "            if batch_idx % 1 == 0:\n",
    "                g_optimizer.zero_grad()\n",
    "\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    refined_images = generator(coarse_output, masked_images, masks[:, 0:1])\n",
    "                    fake_pred = discriminator(refined_images)\n",
    "\n",
    "                    # AGGRESSIVE HOLE FOCUS:\n",
    "                    hole_mask = 1 - masks\n",
    "                    valid_mask = masks\n",
    "\n",
    "                    # 1. Adversarial - let it be stronger for realism\n",
    "                    g_adv_loss = hinge_loss_g(fake_pred) * 0.1\n",
    "\n",
    "                    # 2. Hole-only reconstruction (PRIMARY LOSS)\n",
    "                    g_hole_loss = l1_loss(refined_images * hole_mask, real_images * hole_mask) * 200  # HUGE weight\n",
    "\n",
    "                    # 3. Perceptual loss ONLY in holes\n",
    "                    g_perc_hole = perceptual_loss(refined_images * hole_mask, real_images * hole_mask) * 1.0\n",
    "\n",
    "                    # 4. Edge/boundary loss for better blending\n",
    "                    dilated_mask = F.max_pool2d(1 - masks[:, 0:1], 5, stride=1, padding=2)\n",
    "                    boundary = dilated_mask - (1 - masks[:, 0:1])\n",
    "                    g_boundary_loss = l1_loss(refined_images * boundary, real_images * boundary) * 50\n",
    "\n",
    "                    # 5. Small penalty for changing non-hole regions\n",
    "                    g_preserve_loss = l1_loss(refined_images * valid_mask, coarse_output * valid_mask) * 10\n",
    "\n",
    "                    # Total loss focuses on improving holes\n",
    "                    g_loss = g_adv_loss + g_hole_loss + g_perc_hole + g_boundary_loss + g_preserve_loss\n",
    "\n",
    "                # Use scaler for backward pass\n",
    "                g_scaler.scale(g_loss).backward()\n",
    "                g_scaler.step(g_optimizer)\n",
    "                g_scaler.update()\n",
    "\n",
    "                # Update residual weight (optional)\n",
    "                generator.residual_weight.data.clamp_(0.01, 0.1)\n",
    "\n",
    "            # Update progress bar\n",
    "            epoch_g_loss += g_loss.item()\n",
    "            epoch_d_loss += d_loss.item()\n",
    "            pbar.set_postfix({\n",
    "                'G': f'{g_loss.item():.4f}',\n",
    "                'D': f'{d_loss.item():.4f}',\n",
    "                'α': f'{generator.residual_weight.item():.3f}'\n",
    "            })\n",
    "\n",
    "        # Validation\n",
    "        if epoch % 5 == 0:\n",
    "            val_psnr = validate_gan(generator, cnn_model, val_loader, device)\n",
    "            history['val_psnr'].append(val_psnr)\n",
    "            print(f\"\\nValidation PSNR: {val_psnr:.2f} dB\")\n",
    "\n",
    "            # Save sample images\n",
    "            save_samples(generator, cnn_model, val_loader, epoch, device)\n",
    "\n",
    "        # Save checkpoint with scaler states\n",
    "        if epoch % 10 == 0:\n",
    "            torch.save({\n",
    "                'generator': generator.state_dict(),\n",
    "                'discriminator': discriminator.state_dict(),\n",
    "                'g_optimizer': g_optimizer.state_dict(),\n",
    "                'd_optimizer': d_optimizer.state_dict(),\n",
    "                'g_scaler': g_scaler.state_dict(),  # Save scaler state\n",
    "                'd_scaler': d_scaler.state_dict(),  # Save scaler state\n",
    "                'epoch': epoch\n",
    "            }, f'gan_checkpoint_epoch_{epoch}.pth')\n",
    "\n",
    "        history['g_loss'].append(epoch_g_loss / len(train_loader))\n",
    "        history['d_loss'].append(epoch_d_loss / len(train_loader))\n",
    "\n",
    "    return history\n",
    "\n",
    "# ============================================\n",
    "# 5. VALIDATION AND INFERENCE\n",
    "# ============================================\n",
    "\n",
    "def validate_gan(generator, cnn_model, val_loader, device):\n",
    "    \"\"\"Calculate validation metrics with mixed precision\"\"\"\n",
    "    generator.eval()\n",
    "    total_psnr = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            real_images = batch.to(device)\n",
    "\n",
    "            # Add autocast for validation too\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                masked_images, masks = create_mask(real_images, mask_percentage=0.025)\n",
    "                masks = masks.to(device)\n",
    "\n",
    "                cnn_input = torch.cat([masked_images, masks[:, 0:1]], dim=1)\n",
    "                coarse_output = cnn_model(cnn_input)\n",
    "\n",
    "                refined_images = generator(coarse_output, masked_images, masks[:, 0:1])\n",
    "\n",
    "            # Calculate metrics in FP32 (outside autocast)\n",
    "            mse = F.mse_loss((refined_images + 1) / 2, (real_images + 1) / 2)\n",
    "            psnr = 20 * torch.log10(torch.tensor(1.0)) - 10 * torch.log10(mse)\n",
    "\n",
    "            total_psnr += psnr.item() * real_images.size(0)\n",
    "            count += real_images.size(0)\n",
    "\n",
    "    return total_psnr / count\n",
    "\n",
    "def save_samples(generator, cnn_model, val_loader, epoch, device):\n",
    "    \"\"\"Save sample images\"\"\"\n",
    "    generator.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(val_loader))\n",
    "        real_images = batch[:4].to(device)\n",
    "\n",
    "        # Create masks and get CNN predictions\n",
    "        masked_images, masks = create_mask(real_images, mask_percentage=0.025)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        cnn_input = torch.cat([masked_images, masks[:, 0:1]], dim=1)\n",
    "        coarse_output = cnn_model(cnn_input)\n",
    "\n",
    "        # Generate refined images\n",
    "        refined_images = generator(coarse_output, masked_images, masks[:, 0:1])\n",
    "\n",
    "        # Create comparison grid\n",
    "        comparison = torch.cat([\n",
    "            (real_images + 1) / 2,\n",
    "            (masked_images + 1) / 2,\n",
    "            (coarse_output + 1) / 2,\n",
    "            (refined_images + 1) / 2\n",
    "        ], dim=0)\n",
    "\n",
    "        save_image(comparison, f'gan_samples_epoch_{epoch}.png', nrow=4)\n",
    "\n",
    "# ============================================\n",
    "# 6. INFERENCE FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def inference_gan(generator, cnn_model, image, mask, device):\n",
    "    \"\"\"Run inference with the trained GAN\"\"\"\n",
    "    generator.eval()\n",
    "    cnn_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get CNN prediction\n",
    "        masked_image = image * mask\n",
    "        cnn_input = torch.cat([masked_image, mask[:, 0:1]], dim=1)\n",
    "        coarse_output = cnn_model(cnn_input)\n",
    "\n",
    "        # Refine with GAN\n",
    "        refined = generator(coarse_output, masked_image, mask[:, 0:1])\n",
    "\n",
    "        # Ensure unmasked regions are preserved\n",
    "        final_output = refined * (1 - mask) + image * mask\n",
    "\n",
    "        return final_output, coarse_output"
   ],
   "metadata": {
    "id": "f2-P0hTq05o4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754625696295,
     "user_tz": -120,
     "elapsed": 37,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    }
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# USAGE\n",
    "# ============================================\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize models\n",
    "generator = RefinementGenerator(in_channels=7, out_channels=3).to(device)\n",
    "discriminator = MultiscaleDiscriminator(in_channels=3).to(device)\n",
    "\n",
    "# Load your pre-trained CNN\n",
    "cnn_model = UNetSR().to(device)  # Use the same initialization as training!\n",
    "\n",
    "# Load and fix the state dict\n",
    "cnn_state_dict = torch.load('/content/drive/MyDrive/ColabNotebooks/Vision/inpainting/final_cnn_model.pth', map_location=device)\n",
    "\n",
    "# Remove _orig_mod. prefix if it exists\n",
    "if any(k.startswith('_orig_mod.') for k in cnn_state_dict.keys()):\n",
    "    cnn_state_dict = {k.replace('_orig_mod.', ''): v for k, v in cnn_state_dict.items()}\n",
    "\n",
    "# Now load the fixed state dict\n",
    "cnn_model.load_state_dict(cnn_state_dict)\n",
    "\n",
    "# Set to eval and freeze\n",
    "cnn_model.eval()\n",
    "for param in cnn_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"✅ CNN loaded successfully\")\n",
    "\n",
    "# Train\n",
    "history = train_gan(\n",
    "    generator,\n",
    "    discriminator,\n",
    "    cnn_model,\n",
    "    train_loader=gan_dataloader,\n",
    "    val_loader=val_dataloader,\n",
    "    num_epochs=80,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Save final models\n",
    "torch.save(generator.state_dict(), 'gan_generator_final.pth')\n",
    "torch.save(discriminator.state_dict(), 'gan_discriminator_final.pth')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "d89840c266e24ef78d7d65a196c4ffa4",
      "a865c737e2944dbc9da8e5a4a57051d0",
      "da949447e93a414ca2a3d6e3788be4a5",
      "e8694e6923434f3eb823442b51982d82",
      "cf257cac8c5f4b298882f64a4162cd52",
      "383a7593d3e346df92573c3fa159de4c",
      "8e2d1fe51df84a5ea9343e6d13a13136",
      "c334e8ca0ecc47ad8f4788d357111218",
      "de689220d48e474a83e2c6a34b01f692",
      "ab82449e4585468c8ce4a21f2d2435a9",
      "a0f45abef42a45e48ca664e2b36a0df9",
      "ca2a5cb20d2948ad90b828d73f40c2c7",
      "7e4d2d1e5a4c4c4fac427f46e9f2bc7f",
      "9661e5d1c15c4635bff6d946795a5672",
      "d9bc8fd1639e484caeac3dd248fe7543",
      "b342c35e7fc84be1b01828cf07881316",
      "37a11764082c43dabd4437c67d5e0424",
      "0752aaf4e2744a91bebe4eb994e6456d",
      "aaaa3bf2782f404481b3edadf09088c1",
      "2c6b38d24b034fdfae1e3d25f39cc70c",
      "b48f03043dcf458dab661bf00572f76c",
      "e05f8bcae3f646e9ad2a9b218b359e2b",
      "45664f4e8442455c8a0873002c6eb5f6",
      "7c78c99d540f478e83479399029e33be",
      "d502220102094423a325299cd88dd30a",
      "6116ed8854f442c18a2f3c5635353ec1",
      "f212b1bcbe4c4f3ba3589419ddf9b877",
      "76f37db6c413471b80ccfe5c44d9c875",
      "1754c361997841e9841a5dfaae392d5c",
      "cef34c12618d4c9383b3c2423cd252a1",
      "22323f72217746d799781b33a6e745c9",
      "623b6937679a4040b2a371b92113a1cf",
      "de45bcc2b9c346dfad2e34bb3fb6b296",
      "706ef876220c4fd8a67a21d2fe903d27",
      "c4cbef4a0aac4587b1e193e27f99e5e1",
      "b0d49e53f21447b9835c36bc9d8f6e8c",
      "5572034d0bf14ca4adb600f8feff367a",
      "2d40e6ce3d8b4144abab1929aeefb3d8",
      "211a8fe7d3594b28b4f52f0938cc1bc1",
      "50cfa09b5ea94930895dc5a8a6f0557a",
      "a43feec37524494da424f9894444a88e",
      "ecc97477900e4fedb27645ad5b8818c1",
      "24bb2370f911479283209f76e5682ec6",
      "5922d903d32c4a74931eb8e4134e01dc",
      "6ee0666ca7ac4dc8b3fe4384688df965",
      "57c80859daa04331a3e8695bef071462",
      "1325d4ca97da48c3a3414688ddeca2fa",
      "019f6fd4d1694d07a65fa51a09550a63",
      "8bad56271ed34d2897bcb2d2a5ce029b",
      "dbe30d167c534fe9adb5dc06cbfbb447",
      "1d8b16c7d6f24d8988c069d37134b56c",
      "a9d094ad7f4a446f9ec4d46fda84e74c",
      "b108191ffe9e43a188f932c252297fa7",
      "c6b4a3dd5fd448e381252e2b2615d196",
      "acf98b7432404784a1b0ec1a105dd607",
      "f518fd886f3d4afc829f7cf2db7ff316",
      "694c2cee0b2c4bfabaa9bc5fba47db08",
      "86c5019565b44e1fb816b53974f5d7f7",
      "6f7d15f9f5424d9ea80050d6a4cdec9c",
      "9394471a90964547b2006e4e83f20b48",
      "7ab7bd5410fc4409a405c6984fdde0c5",
      "c24de373923245a2b430c7ff7788cca5",
      "5d41857193df48b4bbcc6e0e26967103",
      "fbc4176f27fd465181b7899ff8f698a3",
      "daf5d2e57c8947caa658940966ff9c46",
      "368c0b72cb8940ba905fb2628d3567bf",
      "978fb1e867fc452a82ecd028d04c2863",
      "ee6e4f1593244a9f952730d174ab4fdc",
      "7ead00763ef0432fa4e671f721458379",
      "27684c596a5d4e7984d304995808899a",
      "a9cc39ab97ba43e78856712cfab8e9a7",
      "e684e5a102134a8d8acb8f2614721e6b",
      "6b5332948958438d821ed3ca717d6be7",
      "d3aaa06e944849b6845cdd8713a7aef3",
      "d202260576dc436389c2f3eedb8754de",
      "f40d371e42044cba928003b92424f532",
      "5d4635959bb44b30aba50ab132aab02b",
      "c5214f6cb3db498d850f18342b06a11f",
      "0dfb6a717fc148149f55605011f4da70",
      "8a84be4d54fb460bbcf3fb3ce0ec2f03",
      "fd6b4500f5364b4ba6a730f285969bcf",
      "23b3afdd74b9458b860459f388f11ae1",
      "eea46237b460478db035018f72bc3ddc",
      "a12018a0dda243f1940ee68f54b4fb65",
      "77e1f559802744ea9f4bfdd83b99b671",
      "2c5b51389c4240978c8dac1db47246f7",
      "dde87515469747998a3b43698bb7c0be",
      "4ae75f8bba054d10aa60a56942a09399",
      "a2d86b3f91ed4b92ac79487062e588be",
      "219c2f7e81de4930ae585e63018f3e2a",
      "07285b033e6c43c88a8d475e285f9521",
      "06292edf743f4ba78ce08c8204570451",
      "70d1cfa2b0f04ab0a26b7b0c6dd28fa0",
      "d7282240e2b8448ca02277da1e83ca6f",
      "173926a072774c4f9f5ae900f8867dbf",
      "c0051b43219041a48a69ca84605b082c",
      "4214479f9a0d495b93ddba0a00e4c576",
      "25e68c5feb3b40039b447caa628b3878",
      "a39edcb3bd2045389b020502c01cb992",
      "5aec2925faee4517aed1c30f8ce29bce",
      "ff4b08d7edc641fbbeeb50c402c1810c",
      "6a8add176bc64001803b84d72f5b0c40",
      "a8e01e69d36c433690d6dd94698230d8",
      "fa0a66b79b7f41d392bd23b708ed7b6b",
      "502c97d968bc4cfc97808d21fc42107d",
      "d5b11553cd7f4b7586377b235f181711",
      "cd7540706ac34ea2bba4f6af7f2f77d8",
      "440d809519d848f2af1d5d7566ddd7df",
      "e4a86c0b12af41aebbe27b6f07209971",
      "d117d6c86b1e45e6bf5f44ae87880fee",
      "7627615ea8964704bdf407b725804ddb",
      "639d3e55c5ab43ecbc1ca65c495a1be8",
      "a5b7ae9da67d435f9daffafdb33de344",
      "fe4961693fb4421a8466250240e29a27",
      "92ecfe5245424672a80535e1c3f40ee6",
      "4b0690df01b043e0a15632eeb94fa62d",
      "8ef8df7fc9c54b8aadbfc365fa6271f0",
      "610ad2fc71c0435f9fe7aebac498b6cd",
      "027e53be6d1b448dbed065b8f38f24c1",
      "a76dfc3cfce74feea4f8fdfe94ec7137",
      "4183147e975646a68ae0f45b09228687",
      "abf33076bb884ab9981dd5755a6a5e5e",
      "c30be069cff24c0aa76854bf673cd0ea",
      "8f6b6a792abd47528bbad399f6fe7a71",
      "806db1a2b7ec4bb9afd1eed1d22502ff",
      "8ea0f635cf244c43b356e9a38046a804",
      "5f555fb4ab0248d49995fd1f32231ac7",
      "a4ebd539a4564892bad33118c224e35c",
      "cea2404647b942e580ed5a0072624dd5",
      "bfb977c4ebf941f0ba5899a022163272",
      "ac0183c12ce145c29b6d75e52685b61b",
      "1f418cf5cd5b437a883176364f525138",
      "e1f195a3cf6c4f86b6dad0b35b6a4f85",
      "dd40f8608a47448a8ed243d881b6f4e3",
      "5a79f04bb018474895be3fa52be9df30",
      "325d7dd6883447b0b10b9921f924353b",
      "7ef79705fee34b61b7c1ff329d39abf4",
      "59b53b64a655449e93e357cf10549dc0",
      "5195c6789e0a4c5d9784012430fffd4f",
      "5424a35ebda243418ff003d2302df531",
      "4c0f4a8694f245728a95be790fdbe89d",
      "aa798b0038f64314918b4cb07da32b97",
      "6aa778a1a3ec4bb08e33ec1164de5fea",
      "1c990b928bcb4297a84349acb068bf22",
      "87b7d01f40d742089b7ff89e974ecef2",
      "65d7e44512ba4f31a41976bad0fb64e1",
      "f2e004a244194fd380d7657d2760a439",
      "a75e791291004387b9ca340fcbd680b4",
      "4d75a4d001a34dc8a7c4cddec39f4f39",
      "d35fecb420094792ab37a658de521e05",
      "1670d7c406cb4737904e8e7ae6a1e947",
      "26579ea77d2d464fb93ed63fbea56936",
      "c8c9829339964f87abb1d670f7e843f3",
      "9d06d66222b74576b11c0c8564713982",
      "3b069429ea4d44748c8ba4de130a7b84",
      "de1c9f92d86c4839aee7abb0678b0718",
      "80060485f2f14afb92fd9522bd798a08",
      "f00bbc5f61d241429418628038fb4921",
      "58178c39bc96480387716247dfa0bb27",
      "ab4c2741710942a6b5a9cf1279c61f72",
      "8ace05a900cb459daa28edd80082cc88",
      "6d5c83ae273f4b349d680301f30ac51d",
      "786fef2834d2454ebbd0e0a38ec163b7",
      "ab35ffe6b95f4feaa506e4111da2f24d",
      "3e8ff867be6d486689a1149c34c60b8b",
      "5179b324f63d4535a09cdfd528893442",
      "6537d5b184fe4dd3a2d09d63401cc83d",
      "a674156e1dae4969b319ee95aaa71b29",
      "617e814420794443af2aeaac47f6ab2b",
      "b0c684682d9245f987ecc8e5dc2fbf97",
      "9513a27c061940bb9d9b89a3b8c54b93",
      "5d53d5d2a5d14ffd927f104d6030bd9b",
      "039af73c7a794068862d9ef1ee2bbfc5",
      "bb05eeea83f44359a6a6401659c825f8",
      "846e79528ad54056b2beb74a5cd72293",
      "5ffc6e6d2132472798f28d2b751a2617",
      "fd9f491fc07c4f0bb16bbfacb7cefe79",
      "6a7420c2a9ca45cbbe91ec7ad07d851e",
      "e8c213fe86424c6f84daeb0e4b0ed2e0",
      "6e392b34d34243db8f797b87e795aaa5",
      "d50f89ef39224e2db4c8b8d96730d44f",
      "07ee7ce80642476dbfab0ac6837606cd",
      "746c35d233144674bf2cb45dd16aea7b",
      "ac32227c498247b9b41912757442562b",
      "f75a2f79d1a84824a4612b15d9bbc6a1",
      "73fbc5b2b5204066974900b2f67ba8c3",
      "cdd36de79fdc412493649a1f2f7ac748",
      "32bf737ad22f46c38f2ffe189e6f77f7",
      "f2a9b7f1985446fe95ad69501b2dcfcc",
      "0f5d441286af46a281f0611c2b031ede",
      "2cda75e3e54e44999c959eaa2a1f30e5",
      "ca455d7a576c483486d831d25b3b794f",
      "7abfbc943feb4c44953adc6247f69d88",
      "3c2f1f7fc96943d184b588a49dfb7a86",
      "57d0feca88114449922e4cee9860ac64",
      "152d20569bdd445cb94671426c3bc913",
      "82a93d7f2d454c02b9b07517487b9a9a",
      "15c610e67f0c4197bd8421c061271fe7",
      "b1ad89269a8c4420a06a70f1273554f3",
      "691815e765b2417e9324d5fbf1911c62",
      "f18339a207a14587a5cc65db183c7df3",
      "e25486b75f7746bb9878c847b6373c1f",
      "c4650dcc0a494c32a6e642c4a7361d16",
      "882c152d91c442e99fb08007eca5b8ac",
      "698db3e0cef743df97d8a49752dc75b7",
      "756ac53dbf5144f29698104addd0f7e9",
      "13099935aaee4acbbcdf4fd76260fc24",
      "3a046f2bc2604cd6949829a6060fc5bf",
      "54189c2b0ac7440d92a8847a21fbee84",
      "6f6483d494fa4dba98f3310a66422f84",
      "9ce429a40c4543459c8af14de02efc79",
      "b7c8e42212374411bcf402a75b53327a",
      "9fe26f7d5d364ff9bc84cb29747e8023",
      "96aa8f3e44ce48eba4bd1a4f4605491e",
      "a53ce61c977d4602806f5b41fb59494f",
      "0e99abb8f75a4e618f1bbf121f9f911f",
      "bf65531390a8459b991119709f496044",
      "a1244ffeb40441e591c855d76757bf72",
      "9ba722e2efe948858146570407bdb75f",
      "d06932eafc1b493e9b0c7cdbd54d4767",
      "1b9e4494a47746d79ac8f700bacc91fd",
      "58329c7c5faa4046b9285add3c9ab9f0",
      "2033468cb95346ba9b203e04607c3aec",
      "4f118acf4f2c44a193eb36c0a06ec88e",
      "9d8db52c34e947bd9821ef8ce9c00087",
      "535b75dad93f4429844996a2cde243b4",
      "9dbf80d13b91492f95a1897386727050",
      "0d9f55f180074b64886c16731008963e",
      "c4d90b4e644d443599457e277646a2b0",
      "633021a325dc4c49aaf4cd2077f3c89d",
      "d56aa4cd892b4de083894a1659a1a169",
      "36a08beacbba4eaf9b5bd70d5aeca3ae",
      "83eef7dbd4ad45a9a46b6fcccf0774fd",
      "8348a22e702940d096b5de44da22cd4d",
      "ca6b98b3efc442a4b627e4daef05be9f",
      "d46bb99ed708424a8dc2570d459a7081",
      "a7e4c6bbdef4407ab1d899f3579f5d9c",
      "17a35159082f4a4bbf72ac05f578275a",
      "d065bbff9d904fe999729e7ed7624d4e",
      "4dfba30f1617413ab4a0a64a9387105b",
      "6d4ee89a95a5422e963f0b7794a22840",
      "c87e020eaef04e02875ac25defe52d8d",
      "54f83e1482e547bba7e340215446dd39",
      "18c21c857774455cb02610de1db2ba64",
      "12da7bc3b419413cb42aa7a8784d17d3",
      "c291baffba8b46ce9480a164c91cb087",
      "6f276d6f6bd64b499bfba5008a1789f2",
      "49f34fcff6c3460d9764f2139443c425",
      "329d41cb4c704557b38d01040260f071",
      "47da1338f69e462c905830ba2dc7dca1",
      "7b193231645f428ea1f8b1e0270a75b2",
      "07e8cd72baad4e7188c8d90dd444c1e8",
      "8e0551a0045041ba8325430657b7be25",
      "36b73fe2a4374384b89e78895ce222af",
      "91a9ea1afe7b49279a400574faa4f90b",
      "2ff9d1e943224a378b9462e797adc297",
      "677f786e2a5542f1954489c7711dbab6",
      "5d4be8354b9b42d3a00c4bae76c4cad9",
      "8730527800f246c8ab452e33439443a1",
      "2ebedc2456a74a059e20cc099357ace6",
      "daf61ac799274b1f8aae0acffc88aaa8",
      "c8c35791aebc48b78e9c19062327f41a",
      "cde17fdb0bee4ad5b3e5df5475fc2b1f",
      "a686e7cf862e40fd9c073c76be124a99",
      "91edb85d4f9f42688dc43653e36cc808",
      "7f18f03c3a604c789603caa224a184f8",
      "12119270937547c2ac4582f79991cf03",
      "1f3db9ef7ba5421987eee52a98fa6fd3",
      "d332b82371394cacb7dfbeae8e34da47",
      "ed3e7cca43e44bd484c8540678ba9c3c",
      "2df51bdffd62410f9a318151c6c6b87d",
      "4d45702c72d7498ca1dd221ac9434b5d",
      "0049be98f31a4f3f9674782fe88dd0e8",
      "1acdb6979b8f4aafb60c66faaeae1992",
      "b10e5419d2c54849992584d964f8ff99",
      "3b61c1a1187c4e52aa81a98efd92b3ae",
      "c6b0a7d25ad4465db08698c55fe6562e",
      "f33ff77f219c482196d0ac1c00a8add6",
      "d4cb3a65e2c44d079bf924f59d5c9642",
      "a5bd247a89b44d6ab80b49b83a9eed50",
      "49202c693e5f44fabe4e0355538ff1e9",
      "6974617624f84d4d8fd4f7bc87d98dfc",
      "e702df3e67044b24ac0ca0582d651dec",
      "585271f856174eafa12159a48cb52f12",
      "dea444a1351c4ce5ad5cff3252d57aea",
      "bb0b25c8851f43bb89420c7b15aa98e8",
      "6f1abba295df4160b1661835235e03c9",
      "e30dfa5d08f644d99d306a25152d7ffd",
      "e1ac8d6b238641f0ad9a1dd4b90bc162",
      "2c5e8d0afb634f2387e2fdd17d8b6c52",
      "b5544579e9a742d3be5a2f583f7d0fbf",
      "a69ab6eb73064be89c611832d8e821f2",
      "a13866349b684eb4af1bf5d686b17cf1",
      "8eb578f07dbf46df8fcf3a0ee7e49b83",
      "d1902433797b49a3aca6501b9abe100d",
      "ae16cb74feba4f45a0ea6b459d4a2a1e",
      "1ec00e019ab34953a58871f519a84643",
      "236408157aae416faeb2e6a4f98248de",
      "bf3f94b63f32419697fa38fba315e28a",
      "2c32e18dde0f41639c105bc84775d6d8",
      "e9bb9d384d8841a5aa66c0600996daf7",
      "5b46d8198dbe40c1aabeb75d8592e417",
      "2860f9f377234659beeb2e1ce231a4d1",
      "d07cdf930970458d9ceb562550d805ee",
      "4dddeb3df1f64da2aac9813217fcd0bf",
      "c8820c39533c477092044ccad1e537ae",
      "103550158539467394075b443800d02b",
      "fabd5b6f405f4d5ea421f34fe26a163d",
      "23065c8357c140abb2629c703e2e9967",
      "8f666b6101df4ea4a38dd10a477bdb17",
      "8617105680d7477297eafb90fba97629",
      "127b0898b0d7449cb9858bada79b5ef0",
      "78870b0a166d45eab152fdf2d8227574",
      "769cb791aae64e3c8fa2dafc11e64921",
      "fad57abe6c60419890f79caed879172d",
      "254a1befde9245fd8ec2fca861464056",
      "00efab87131045688926fdd98bc7cda3",
      "97bffdbc9778453a8eff3f233a4e27da",
      "a8e642dd132c41b488423d6a75a67f00",
      "fc89ceaf24ac443995d60245f9f10162",
      "2f89f38b5fbb4649bf4ae9991b99a77d",
      "ef3884a6b6c94c569887090c6b9faca6",
      "cadc81edfdab434a86de96f821420c13",
      "27fb1eef2c4a4cc5928eb2f43429d81c",
      "5965c1432eb943d793d7c1ace4300708",
      "1bb501f124cd4bfcbfc1adc327690be3",
      "e6b1181781ab4b6ab05bfa932e1998bf",
      "63674e784c4d4a70a9c3c41546ea53ce",
      "3cede59fc8584275a75123200cac4fd9",
      "4e978aa96b2f4826b7c58cf4551dfae0",
      "3f9ba9b0bb444b1ca7520b2f7a9502fd",
      "56ad7b70442a44148ce09eca1a9c1351",
      "e723ae48a0ec45349182f1575a81ddd5",
      "a4b23303164b4737be1a617ecc560541",
      "ff203ec0f14b4833bf9cd4148cfc38cc",
      "2c7382abf5a442688aa57509a01290aa",
      "356c84684e9f4d20960645d76f5af9a9",
      "1951c7625fcd41638bca681fddc91ea1",
      "8ea097127ecb4b1480d4d6c85beca2f6",
      "b972a046990f4b12908dc7b162b3b125",
      "757973a745054769b7da0e1788267b2c",
      "9127ccb924b0476992bd251796bb3847",
      "9509fb6a92b74e85aadc66418738b96e",
      "6719f8d2a47d4658b77888d0515d1ffc",
      "1ce2a8f905d54e1ba5712a57dab17133",
      "286e48209f3f4a928c6550d8fe75d483",
      "9f62bad24dab4b2bbb825e02bee269ab",
      "4574b875ad0041e7a580b2770d7d74b3",
      "05851a6acf824edbbbb4c3a9c9fa374b",
      "dc2dbc7bbce64973a7a03822846da2ad",
      "60c0db5c6be94967ac517c8c4486923e",
      "38b75ff972e54499876ca8a02384c17f",
      "ed85ac6632d54f6989ba4669bae06c34",
      "fa4672ec380e412096f07fc9c5df8293",
      "57bd0dd0d56445e38da037c61dfe71c3",
      "3c2fb52d619842509e84cdbc0ec380b5",
      "99c8a16e7dac4297a4707a78b244e6f4",
      "133fc191cef341f3a50d2ab5cdcfa658",
      "5cc1bd7924364760b11995e928e3ad25",
      "0ced31c294af41f9b0f1168a3d4e96c7",
      "9919f87684eb472f8c762229f391210a",
      "4b2965ed6dbf41b6894bdc11fbfbead8",
      "ec6ebb0be5324f1c862e9c2d9f681f86",
      "e9acf88e7ab846dfb9337576e31186e8",
      "e732af85aed643fb88429b101949b781",
      "9af3a5ca33cc4a768acc3c56915227ef",
      "d352741b0deb48af8729879c8ea75004",
      "ab3540c0adfb4fbfae8ee4e7b99b5b4b",
      "9bccc0d5cd1f410e8fe872e3771133cc",
      "fa9bbda6c75b44d8acbd516c1ca0d551",
      "8fe0e931e3d049febcf81e42f63a2c5a",
      "8ef98e900a4c48ddb95b0bdf6d494497",
      "4923c065905e4387b7a60ff71c0bbdea",
      "780e5d83adf140968edd698b8b3bd640",
      "ed321d09006f45f1a48287014c634222",
      "3b1e064c336445e08fda87885da3fd82",
      "c69f1205cffc44a0ba8198ef8f13ff11",
      "c8263ae57e9f41afad858b2de8f427b2",
      "193c50f7be5b4a8590ddc0d466b1653a",
      "d88599df6c5c4d05b769f0993d10d424",
      "6a466a833ce6416d893035ed484da5ef",
      "dbd923c8cc414eaf857a0d59599f021a",
      "b8cceae23fce45eb896644edfe738f25",
      "74f65abd32734092b4c58c991cfca348",
      "5642dffa84a8452a8d9eb17009feca43",
      "4fe5502b716a4484829007d57167c189",
      "c015be179c8d4d1b9692e2ffc651d239",
      "e9f2a493888446c085cdbe0c9a85fb92",
      "c6e6602789c04848b27842459f8a9ff8",
      "a26dfedc365a4a71b27bd4129ef8f3d7",
      "585640443fdb451f90b10508c76663c1",
      "ea911c3286524223acaa423150bb7c98",
      "54938a3ddc664c61a4c713f3124b6a47",
      "e74da1fc25ba4b49ad9121b193a720d6",
      "2b0831087d704a4c9326f026a0c10da3",
      "df58d1d2586244efb168ca72c1791518",
      "322ea6a833ea42bb9bd88950fcd32e58",
      "68874a481ec84d389a484f7d708b4312",
      "bbe86ce5a8a843a08c583ad27a8cd9ec",
      "7c33b9f6772a49d59f74f8c3ba76f19d",
      "43fa6f7cb9a0425e9b0dcaf8e8a38620",
      "36bc0c706d954e46a1748835b7ca6cfd",
      "861956f3086146f298bc9664aec2ca3f",
      "68c098ef0f234c89be9e15e3445ebee3",
      "dbbf328f533c47adb8eeb1e8a3346963",
      "72a70218b6964e85b6c0ddc8296ad313",
      "e644365dc0c2409d827e36f496836905",
      "9e6058b7601d4eb799c80bed3ec872cf",
      "6d6d3867b9944d889ba01781c3107eca",
      "5bd4c11f28b445ada4eae65854e62d44",
      "229d370e364e4102a7805db437cd59c0",
      "11260afc147a490fb4439da2ac2dda0d",
      "805bb2f78e6e4fc9ad432f2a9112b37f",
      "db7e20f2772241a68091c69005289ece",
      "aa536181a9644275bfab459e3a4adc8e",
      "50ec8e6db1464eca81f15441d9062deb",
      "e2a172862f9441d2ba73f4306689f1a8",
      "40a666773b6e44499afaaf5d4345cd18",
      "66d5d9d665ad48d7ae0d9d20faa007f4",
      "2729839ee22c4fc69b9eeced9d753b5e",
      "5bd2407b91d042c4be19faccd9cddede",
      "0d03f44b1a474e639c2364b7448f5742",
      "a120e99cd9c64aaca4be7bd90c4226b2",
      "48396ce9afef4ddaa46373e8e81f24af",
      "1095418242f34a27bf573c19f33b1a1c",
      "b5889414c831432996be68023040fccd",
      "65de5c86d44748c29e6dce119860d53e",
      "57c4d912274e4c25a299dee3d6b8ef47",
      "7342123ea55346e59f003732bd6a2985",
      "fb2883aa329c4a658d64e579069cb9b7",
      "76a6f2e55e52473a8c4290d179de772e",
      "33b925d2eceb4f28a862d2cca1568cd6",
      "2ca34fdeb7c54e65ab58d2143c22becf",
      "7c1b31b62a174ceb88d521329f64b968",
      "d8ad27ac4f3f49ae9431bdd8237faab0",
      "08d67e9fd31f445b81c21c1f72fa7113",
      "d8bbfee5850645b886ec178037cd4662",
      "6102114cc00d4603977dedc826c62b43",
      "930005996c214dd0b338acf323aabac9",
      "ff07fd75a91a4fc584f5039ffa3fb6b8",
      "f4002bd7de0a4d33a076c8dfd31aa9c7",
      "344d63ff0df0498aae13598f30f54d0c",
      "5e02c67bd1b9415d960e39947f814c07",
      "f9ccc94056e74612bdb2f3ba6ecb0d78",
      "01fe765e13f743b6aaa0bc62f576bb9f",
      "d01efab620a24d658becd5f312741a90",
      "8af43c4baa554d94b98758977eb76a19",
      "65e4717535114154a008ffd8b4b57d5a",
      "0dcf692e2c924986a7738fcc2a871e4d",
      "349065d0828a42caad7b8d3cc26c5afe",
      "a366e2e9f8f44e5fb57dad706f78991e",
      "d5156f375e154b29919cb798ecded135",
      "55abc576513748729e057855f5f76853",
      "60c8987753854c0285f462b7d7d235c2",
      "545177419e8445ee82a49dc917f83cc1",
      "00359fc81db64e908ad2a8b15dac64df",
      "2436c2e0f5ea40449bccae0f9b64dbce",
      "695c0ec265154e53b0852426f377d444",
      "5e428b7bce0740d287a27b4282ab6ea5",
      "f6f52a2a78fb4bbe86bd63294e69678d",
      "50f9f741cefb43629a341b3f9cb7daed",
      "da72ea09c7a74c4997567110dfe71472",
      "374ddd2738ec4f9daaafd6a39873fcdf",
      "c561d639901748c8a4391220827ba3c1",
      "6dfd96b36ed74069b10857673b50e590",
      "a1b4e7bf196f4f24b383e6a430d6f7d4",
      "89820f92a54e4f2ea11db480e1505d61",
      "33c154543be74212917476c1c430a9ce",
      "c05fe4de7a744c7890825c06e8f6f2a8",
      "d4966b1e03364c65b2792e59ba2b9207",
      "b65f6eb36e204fd38b2e15c34884eced",
      "7fba7541cf324fc48cad87265a3dafa9",
      "88bf0de0ef6d4a42935b30551d35ac7b",
      "7e2d944315f84909a544edd9282ed3b7",
      "7ac6270a97654ec29d1f3bd351cc7d41",
      "90f846b80476406090750f0691d0a5af",
      "cdc03d9e7a384d1a851db3842bb336ce",
      "7f9bcc18b1d7441ebe3fc88b65fee185",
      "d8a7f0b1fa024651848c513ba8136c24",
      "f4cb29dbc2c2404e8ac087bf3c3a53f2",
      "9365a6054bbb417aac35fb55b26b2adb",
      "7151047f88ba4706935fd2ad38d964bb",
      "27b608a3a343421196f065f891119396",
      "aabc840f125444508c9c56b734b6ce88",
      "210137910d8c4d2ca576cc59ec3166a9",
      "0db1271f7a394be3baedd7b84241ca0f",
      "68b2229f3c224a68956d6de3a14cb23a",
      "1d677df6fcec40b58b46b4daa1b0d9e1",
      "6ca5dfbb7420485681818a15554a0210",
      "94ab0c19a57b46d1bf1d2cb9532579d2",
      "2dfda36cd1ec4693ae1c0061496b6b7f",
      "35c4816ba70c4d1c90a75e73e93cd867",
      "6610fe6fd2c94ec0855928247bcbc268",
      "338d0ba956fc485484fc0f4a9a7cb5a3",
      "339e1b3531e042afbe5cad4d727d5a3a",
      "2de2c383fb6542f59be60ed52bc202e2",
      "2ad8aec452254b608aac756a17ce7619",
      "14ab3d2577a940b4af57f1686f206e24",
      "4dc334c96c934b5cb9f71960eb84460f",
      "b32f7319ce16432385f97fae3e7abb07",
      "156ed6a7245f4de79ae3ac8483bf526b",
      "ddbee412040244da951d458510c7f262",
      "83f76114dc3c4c0a9041941bb1bdf43f",
      "08cea8611ad848a18c39cf60b8f9b806",
      "f3660ea8d80f4970aa9412bdbc52543c",
      "1aefac3828d848a1a5135dd8cd1d6f3f",
      "c295d34c675d4db49b51b2290ceac602",
      "d4405dec60cb4d9ba1bd8326a5eebcb5",
      "3896b3c4ddb74225a611852d42a9c2bc",
      "4b6aece8bac9460fa0b5c27e5b1f194d",
      "5c60c997a6e0406da0c33e1e645eab17",
      "631d81a70db04762af4605322859666c",
      "c00be76c3f104734a04b18382e96e863",
      "e37d903a019340418997bcd9f865b212",
      "fb0b5664456a4923b25d7dbf2159e805",
      "915712105f5247328b83a66a6b5f664f",
      "ed0bf6048e6a40f1937ed8ef6beebb66",
      "41468d007090425c97b9dfa04330ab12",
      "4abf87f5da7443d7945ea6d6d65935ea",
      "a2b2d337dc6843a286d6250c2e33b5b5",
      "e276d774aa5c41c7a4d47c4790fc0c56",
      "39a41a91509a47fc8ec24786ec054652",
      "9a48ebbfc98249cf92780737957109e4",
      "acc66e38b00e4abc9bdfa3ad2e69420a",
      "0d21ad867b554f49893235168bec856c",
      "35fc34024aa842b3b09a2ed4d77a8e8a",
      "ca6015f39d5e4d6b870d838455c71cd0",
      "a98b4d82a39442e58870d3cc9aada873",
      "17b22e45f91a4dea8f6a8687cbf06662",
      "6bb10d4a061541a2b2064ba0e8f9e142",
      "ae8ff62039054abf83429de1824dc2d1",
      "67daf2d9f9fb40709c647a1fce04a28f",
      "32fbe7cf5382400a981f3da978fcb480",
      "145122c8fe234e7d8504f66ff9c84649",
      "a849f048751841328c318ebb63eb3f06",
      "a61e1c6342ec497483e18d16539ad264",
      "d4ec6e459bf648479e6f9c42e572cc7f",
      "1d78240e404244809a7eb130ec02dd6e",
      "c39d50c9d90f484898aea0ea85aacb18",
      "87b18bcc3ee24021ab1a803ed87cc15b",
      "e999b2c1ec474f508fc080b613d18dd1",
      "5a4e6839cc57446390454a610dcb77c2",
      "9acebc7438a7437d958a1729da617f3b",
      "3f5234fa43984ca38f6bd536454dee79",
      "b3503493223b46d2a1a8e9a57ba7e599",
      "057826c9a72e45e1a2a3a6af6cebd108",
      "b8b3edb047d14f829da94147b7116ca4",
      "794b00dffa1d42c59eaac4b382269742",
      "64e6202a54be46b5b3bf526deaf0f64e",
      "6ab02a680c304640a91c42b1ab8fa08f",
      "71fe2ded35ab4861bea5c0625ff766f9",
      "46b49992b93740e087b1dba18f065633",
      "36e63cb27e9d40b4b28dc53cff2f3475",
      "b6443430186f4a64860b4522e3cfc4d6",
      "0b79090df71441619e7aa4932e4325b9",
      "4ab5e461351644428c3c8de913cf518f",
      "2debf71e39b94c34857b60c4891141fd",
      "c845acf3ee0f4620ad3839965d12f46f",
      "e33e205d213e483f9186b5cf16222f3e",
      "019062e85887486e9f244eca4c83f9fe",
      "5bd061e8a8544829849a1a6f6c00c193",
      "259edbff59874e5ab0785b27a7a116c2",
      "cde9ba5445c245fda14b3a283bcc41ee",
      "9baf127f72254547ab9559b6c797b5b5",
      "acde8e48c9d54aa6bf6df7f3b33e5223",
      "27b913b78f484bc49daa9432696ef0a7",
      "420e1fe65f4b4dcfacfe228b2b7f0977",
      "f65eae7b5cc84a569b0443832a6d2c4e",
      "844f980bc8c14a908852448b3dc6f503",
      "587539ed58c54568bd582361c08bdb76",
      "c517afa320be44f7a8b14259624f00e7",
      "da5c8a6c14e14eeabf93d469b23def7b",
      "50820b2a2f144c669fcecb5151ad25a4",
      "cf4fa92eb7184870bcd276e9e8e14252",
      "f04a73bae4d1418a9d072746dbd6d11b",
      "370a59fd3a8b4a55898ed6fb52e896ca",
      "3a5d6e3893144efc9170503b8fc7abb7",
      "21041878cad7455aac0ea7be58f24930",
      "2b9d7a41977148dfbd9b819613ab011e",
      "4d6ec188262640cca29c7922f52b3009",
      "029a74a5bafe4eb8971eac184ef0c547",
      "46e60c05a1da4deb82bf14e49e91c34e",
      "d021f85d4055474091adaa161897f7c6",
      "8e5b620fc97043768bf0f8c5743686e3",
      "9b051b84fe084684977aa841903b9dcf",
      "3dec1f26a23c47528637003824eb120b",
      "9ee8bbca01554032819067a9b402e602",
      "ebbb3524fa5440e2b3c9e97781093e4f",
      "9f479875f23a4c7d87692b8b2a173049",
      "721f2a98f5e34ef69ebb30fed2091a70",
      "6b1aba593d824fd9b583b80d63457415",
      "41ac84f793a1408f8aa5c397402c86e4",
      "c732351cbd364c1db71e224697e4efb4",
      "a268af2a0c514cb4a913b5c50915cdf7"
     ]
    },
    "id": "xhFMxxVK08LQ",
    "executionInfo": {
     "status": "error",
     "timestamp": 1754626420047,
     "user_tz": -120,
     "elapsed": 715487,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    },
    "outputId": "877bb301-b670-4e31-bf4f-8453a37c17ab"
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✅ CNN loaded successfully\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
      "100%|██████████| 548M/548M [00:02<00:00, 233MB/s]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 1/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d89840c266e24ef78d7d65a196c4ffa4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Validation PSNR: 26.85 dB\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 2/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ca2a5cb20d2948ad90b828d73f40c2c7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 3/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "45664f4e8442455c8a0873002c6eb5f6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 4/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "706ef876220c4fd8a67a21d2fe903d27"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 5/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ee0666ca7ac4dc8b3fe4384688df965"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 6/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f518fd886f3d4afc829f7cf2db7ff316"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Validation PSNR: 27.33 dB\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 7/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "978fb1e867fc452a82ecd028d04c2863"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 8/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c5214f6cb3db498d850f18342b06a11f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 9/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a2d86b3f91ed4b92ac79487062e588be"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 10/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5aec2925faee4517aed1c30f8ce29bce"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 11/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7627615ea8964704bdf407b725804ddb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Validation PSNR: 27.77 dB\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 12/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "abf33076bb884ab9981dd5755a6a5e5e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 13/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e1f195a3cf6c4f86b6dad0b35b6a4f85"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 14/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1c990b928bcb4297a84349acb068bf22"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 15/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3b069429ea4d44748c8ba4de130a7b84"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 16/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5179b324f63d4535a09cdfd528893442"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Validation PSNR: 28.15 dB\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 17/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fd9f491fc07c4f0bb16bbfacb7cefe79"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 18/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "32bf737ad22f46c38f2ffe189e6f77f7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 19/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b1ad89269a8c4420a06a70f1273554f3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 20/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6f6483d494fa4dba98f3310a66422f84"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 21/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1b9e4494a47746d79ac8f700bacc91fd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Validation PSNR: 28.54 dB\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 22/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "36a08beacbba4eaf9b5bd70d5aeca3ae"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 23/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "54f83e1482e547bba7e340215446dd39"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 24/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "36b73fe2a4374384b89e78895ce222af"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 25/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "91edb85d4f9f42688dc43653e36cc808"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 26/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3b61c1a1187c4e52aa81a98efd92b3ae"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Validation PSNR: 28.93 dB\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 27/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6f1abba295df4160b1661835235e03c9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 28/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "236408157aae416faeb2e6a4f98248de"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 29/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "23065c8357c140abb2629c703e2e9967"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 30/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fc89ceaf24ac443995d60245f9f10162"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 31/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f9ba9b0bb444b1ca7520b2f7a9502fd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Validation PSNR: 29.12 dB\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 32/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9127ccb924b0476992bd251796bb3847"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 33/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ed85ac6632d54f6989ba4669bae06c34"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 34/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e9acf88e7ab846dfb9337576e31186e8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 35/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ed321d09006f45f1a48287014c634222"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 36/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4fe5502b716a4484829007d57167c189"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Validation PSNR: 29.20 dB\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 37/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "322ea6a833ea42bb9bd88950fcd32e58"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 38/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e6058b7601d4eb799c80bed3ec872cf"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 39/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "66d5d9d665ad48d7ae0d9d20faa007f4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 40/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fb2883aa329c4a658d64e579069cb9b7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 41/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f4002bd7de0a4d33a076c8dfd31aa9c7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Validation PSNR: 29.06 dB\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 42/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d5156f375e154b29919cb798ecded135"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 43/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "374ddd2738ec4f9daaafd6a39873fcdf"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 44/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e2d944315f84909a544edd9282ed3b7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 45/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "210137910d8c4d2ca576cc59ec3166a9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 46/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2de2c383fb6542f59be60ed52bc202e2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Validation PSNR: 29.17 dB\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 47/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c295d34c675d4db49b51b2290ceac602"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 48/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "41468d007090425c97b9dfa04330ab12"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 49/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "17b22e45f91a4dea8f6a8687cbf06662"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 50/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "87b18bcc3ee24021ab1a803ed87cc15b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 51/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "71fe2ded35ab4861bea5c0625ff766f9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Validation PSNR: 29.20 dB\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 52/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "259edbff59874e5ab0785b27a7a116c2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 53/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "50820b2a2f144c669fcecb5151ad25a4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Epoch 54/80:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8e5b620fc97043768bf0f8c5743686e3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipython-input-3700431786.py\u001B[0m in \u001B[0;36m<cell line: 0>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[0;31m# Train\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 32\u001B[0;31m history = train_gan(\n\u001B[0m\u001B[1;32m     33\u001B[0m     \u001B[0mgenerator\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     34\u001B[0m     \u001B[0mdiscriminator\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipython-input-3058908258.py\u001B[0m in \u001B[0;36mtrain_gan\u001B[0;34m(generator, discriminator, cnn_model, train_loader, val_loader, num_epochs, device)\u001B[0m\n\u001B[1;32m     55\u001B[0m                 \u001B[0;31m# Discriminator predictions\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     56\u001B[0m                 \u001B[0mreal_pred\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdiscriminator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreal_images\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 57\u001B[0;31m                 \u001B[0mfake_pred\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdiscriminator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrefined_images\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdetach\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     58\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     59\u001B[0m                 \u001B[0;31m# Hinge loss\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compiled_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[misc]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1738\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1739\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1740\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1741\u001B[0m     \u001B[0;31m# torchrec tests the code consistency with the following code\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1748\u001B[0m                 \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1749\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1751\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1752\u001B[0m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipython-input-839336469.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnum_scales\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 42\u001B[0;31m             \u001B[0moutputs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdiscriminators\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     43\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mi\u001B[0m \u001B[0;34m<\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnum_scales\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     44\u001B[0m                 \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdownsample\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compiled_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[misc]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1738\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1739\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1740\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1741\u001B[0m     \u001B[0;31m# torchrec tests the code consistency with the following code\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1748\u001B[0m                 \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1749\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1751\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1752\u001B[0m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    248\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    249\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 250\u001B[0;31m             \u001B[0minput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    251\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    252\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compiled_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[misc]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1738\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1739\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1740\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1741\u001B[0m     \u001B[0;31m# torchrec tests the code consistency with the following code\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1843\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1844\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1845\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0minner\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1846\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1847\u001B[0m             \u001B[0;31m# run always called hooks if they have not already been run\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36minner\u001B[0;34m()\u001B[0m\n\u001B[1;32m   1791\u001B[0m                 \u001B[0margs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbw_hook\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msetup_input_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1792\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1793\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1794\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0m_global_forward_hooks\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_forward_hooks\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1795\u001B[0m                 for hook_id, hook in (\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    552\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    553\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 554\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_conv_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbias\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    555\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    556\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001B[0m in \u001B[0;36m_conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    547\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgroups\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    548\u001B[0m             )\n\u001B[0;32m--> 549\u001B[0;31m         return F.conv2d(\n\u001B[0m\u001B[1;32m    550\u001B[0m             \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbias\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstride\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdilation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgroups\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    551\u001B[0m         )\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation and Visualization"
   ],
   "metadata": {
    "id": "dlkJP7Iy3lM2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class ComprehensiveMetrics:\n",
    "    \"\"\"Extended metrics for thorough evaluation\"\"\"\n",
    "\n",
    "    def __init__(self, device='cuda'):\n",
    "        self.device = device\n",
    "        self.psnr = torchmetrics.PeakSignalNoiseRatio(data_range=1.0).to(device)\n",
    "        self.ssim = torchmetrics.StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "        self.lpips = lpips.LPIPS(net='alex').to(device)\n",
    "\n",
    "        # FIX: Use smaller kernel size or make it optional\n",
    "        try:\n",
    "            self.ms_ssim = torchmetrics.MultiScaleStructuralSimilarityIndexMeasure(\n",
    "                data_range=1.0,\n",
    "                kernel_size=7,  # Reduced from 11 to 7\n",
    "                betas=(0.0448, 0.2856, 0.3001, 0.2363, 0.1333)\n",
    "            ).to(device)\n",
    "            self.ms_ssim_available = True\n",
    "        except:\n",
    "            self.ms_ssim_available = False\n",
    "            print(\"MS-SSIM not available, skipping...\")\n",
    "\n",
    "    def calculate_all_metrics(self, pred, target, mask=None):\n",
    "        \"\"\"Calculate comprehensive metrics\"\"\"\n",
    "        metrics = {}\n",
    "\n",
    "        # FIX: Ensure proper dimensions\n",
    "        if pred.dim() == 3:\n",
    "            pred = pred.unsqueeze(0)\n",
    "        if target.dim() == 3:\n",
    "            target = target.unsqueeze(0)\n",
    "\n",
    "        # Convert to [0,1] range\n",
    "        pred_01 = (pred + 1) / 2\n",
    "        target_01 = (target + 1) / 2\n",
    "\n",
    "        # Basic metrics\n",
    "        metrics['psnr'] = self.psnr(pred_01, target_01).item()\n",
    "        metrics['ssim'] = self.ssim(pred_01, target_01).item()\n",
    "\n",
    "        # FIX: Only calculate MS-SSIM if image is large enough\n",
    "        if self.ms_ssim_available and pred_01.shape[2] >= 160 and pred_01.shape[3] >= 160:\n",
    "            try:\n",
    "                metrics['ms_ssim'] = self.ms_ssim(pred_01, target_01).item()\n",
    "            except:\n",
    "                metrics['ms_ssim'] = metrics['ssim']  # Fallback to regular SSIM\n",
    "        else:\n",
    "            metrics['ms_ssim'] = metrics['ssim']  # Use regular SSIM for small images\n",
    "\n",
    "        metrics['lpips'] = self.lpips(pred, target).mean().item()\n",
    "        metrics['mae'] = F.l1_loss(pred_01, target_01).item()\n",
    "        metrics['mse'] = F.mse_loss(pred_01, target_01).item()\n",
    "\n",
    "        # Gradient difference (edge preservation)\n",
    "        pred_grad = self._compute_gradient(pred)\n",
    "        target_grad = self._compute_gradient(target)\n",
    "        metrics['grad_diff'] = F.l1_loss(pred_grad, target_grad).item()\n",
    "\n",
    "        # If mask provided, calculate region-specific metrics\n",
    "        if mask is not None:\n",
    "            # FIX: Ensure mask has proper dimensions\n",
    "            if mask.dim() == 3:\n",
    "                mask = mask.unsqueeze(0)\n",
    "\n",
    "            # Ensure mask is single channel for calculations\n",
    "            if mask.shape[1] == 3:\n",
    "                mask_single = mask[:, 0:1, :, :]\n",
    "            else:\n",
    "                mask_single = mask\n",
    "\n",
    "            hole_mask = 1 - mask_single\n",
    "\n",
    "            # Metrics for inpainted region only\n",
    "            metrics['psnr_hole'] = self._masked_psnr(pred_01, target_01, hole_mask).item()\n",
    "            metrics['ssim_hole'] = self._masked_ssim(pred_01, target_01, mask_single)\n",
    "\n",
    "            # FIX: Check if there are any holes before calculating MAE\n",
    "            if hole_mask.sum() > 0:\n",
    "                metrics['mae_hole'] = (F.l1_loss(pred_01 * hole_mask, target_01 * hole_mask, reduction='sum') / hole_mask.sum()).item()\n",
    "            else:\n",
    "                metrics['mae_hole'] = 0.0\n",
    "\n",
    "            # Boundary metrics (transition quality)\n",
    "            boundary = self._get_boundary(mask_single)\n",
    "            if boundary.sum() > 0:\n",
    "                metrics['boundary_mae'] = (F.l1_loss(pred_01 * boundary, target_01 * boundary, reduction='sum') / boundary.sum()).item()\n",
    "            else:\n",
    "                metrics['boundary_mae'] = 0.0\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def _compute_gradient(self, x):\n",
    "        \"\"\"Compute image gradients using Sobel filter\"\"\"\n",
    "        # FIX: Ensure proper batch dimension\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]],\n",
    "                               dtype=torch.float32).view(1, 1, 3, 3).to(x.device)\n",
    "        sobel_y = sobel_x.transpose(2, 3)\n",
    "\n",
    "        # Apply to each channel\n",
    "        grad_x = F.conv2d(x, sobel_x.repeat(3, 1, 1, 1), groups=3, padding=1)\n",
    "        grad_y = F.conv2d(x, sobel_y.repeat(3, 1, 1, 1), groups=3, padding=1)\n",
    "\n",
    "        return torch.sqrt(grad_x**2 + grad_y**2 + 1e-8)  # Add epsilon for stability\n",
    "\n",
    "    def _get_boundary(self, mask, dilation_size=3):\n",
    "        \"\"\"Get boundary region of mask\"\"\"\n",
    "        # Ensure mask is single channel and 4D\n",
    "        if mask.dim() == 3:\n",
    "            mask = mask.unsqueeze(0)\n",
    "        if mask.dim() == 4 and mask.shape[1] == 3:\n",
    "            mask = mask[:, 0:1, :, :]\n",
    "        elif mask.dim() == 4 and mask.shape[1] != 1:\n",
    "            mask = mask[:, 0:1, :, :]\n",
    "\n",
    "        kernel = torch.ones(1, 1, dilation_size, dilation_size).to(mask.device)\n",
    "        dilated = F.conv2d(mask, kernel, padding=dilation_size//2)\n",
    "        dilated = (dilated > 0).float()\n",
    "\n",
    "        eroded = F.conv2d(mask, kernel, padding=dilation_size//2)\n",
    "        eroded = (eroded == dilation_size**2).float()\n",
    "\n",
    "        boundary = dilated - eroded\n",
    "        return boundary\n",
    "\n",
    "    def _masked_psnr(self, pred, target, mask):\n",
    "        \"\"\"Calculate PSNR only in masked region\"\"\"\n",
    "        # Ensure all have same dimensions\n",
    "        if pred.dim() == 3:\n",
    "            pred = pred.unsqueeze(0)\n",
    "        if target.dim() == 3:\n",
    "            target = target.unsqueeze(0)\n",
    "        if mask.dim() == 3:\n",
    "            mask = mask.unsqueeze(0)\n",
    "\n",
    "        masked_pred = pred * mask\n",
    "        masked_target = target * mask\n",
    "\n",
    "        if mask.sum() > 0:\n",
    "            mse = ((masked_pred - masked_target) ** 2).sum() / mask.sum()\n",
    "            if mse > 0:\n",
    "                return 20 * torch.log10(torch.tensor(1.0)) - 10 * torch.log10(mse)\n",
    "            else:\n",
    "                return torch.tensor(40.0)  # Perfect reconstruction\n",
    "        else:\n",
    "            return torch.tensor(0.0)\n",
    "\n",
    "    def _masked_ssim(self, pred, target, mask):\n",
    "        \"\"\"Calculate SSIM focusing on masked region\"\"\"\n",
    "        # Ensure proper dimensions\n",
    "        if pred.dim() == 3:\n",
    "            pred = pred.unsqueeze(0)\n",
    "        if target.dim() == 3:\n",
    "            target = target.unsqueeze(0)\n",
    "        if mask.dim() == 3:\n",
    "            mask = mask.unsqueeze(0)\n",
    "\n",
    "        # Handle multi-channel masks properly\n",
    "        if mask.dim() == 4 and mask.shape[1] == 3:\n",
    "            mask_single = mask[:, 0, :, :]\n",
    "        elif mask.dim() == 4:\n",
    "            mask_single = mask.squeeze(1) if mask.shape[1] == 1 else mask[:, 0, :, :]\n",
    "        else:\n",
    "            mask_single = mask\n",
    "\n",
    "        # Find region containing mask\n",
    "        coords = torch.where(mask_single > 0)\n",
    "        if len(coords[0]) == 0 or len(coords[1]) == 0:\n",
    "            # No mask region, return full SSIM\n",
    "            return self.ssim(pred, target).item()\n",
    "\n",
    "        # Get bounding box\n",
    "        y_min, y_max = max(0, coords[0].min().item()), min(pred.shape[2], coords[0].max().item() + 1)\n",
    "        x_min, x_max = max(0, coords[1].min().item()), min(pred.shape[3], coords[1].max().item() + 1)\n",
    "\n",
    "        # Ensure we have a valid region\n",
    "        if y_max - y_min < 11 or x_max - x_min < 11:\n",
    "            # Region too small for SSIM, use MAE instead\n",
    "            return 1.0 - F.l1_loss(pred, target).item()\n",
    "\n",
    "        pred_crop = pred[:, :, y_min:y_max, x_min:x_max]\n",
    "        target_crop = target[:, :, y_min:y_max, x_min:x_max]\n",
    "\n",
    "        return self.ssim(pred_crop, target_crop).item()"
   ],
   "metadata": {
    "id": "Q3v_lKEa3nPo",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754626423607,
     "user_tz": -120,
     "elapsed": 27,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    }
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def create_advanced_visualization(original, masked, cnn_output, gan_output, mask,\n",
    "                                 metrics_cnn, metrics_gan, save_path='advanced_vis.png'):\n",
    "    \"\"\"Create comprehensive visualization with multiple analysis views\"\"\"\n",
    "\n",
    "    fig = plt.figure(figsize=(35, 20))\n",
    "    gs = fig.add_gridspec(5, 8, hspace=0.3, wspace=0.3)\n",
    "\n",
    "    # Convert tensors to numpy\n",
    "    def to_numpy(tensor):\n",
    "        return ((tensor.cpu() + 1) / 2).clamp(0, 1).permute(1, 2, 0).numpy()\n",
    "\n",
    "    original_np = to_numpy(original)\n",
    "    masked_np = to_numpy(masked)\n",
    "    cnn_np = to_numpy(cnn_output)\n",
    "    gan_np = to_numpy(gan_output)\n",
    "    mask_np = mask[0, 0].cpu().numpy()\n",
    "\n",
    "    # Row 1: Main images\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1.imshow(original_np)\n",
    "    ax1.set_title('Original', fontsize=12, fontweight='bold')\n",
    "    ax1.axis('off')\n",
    "\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax2.imshow(masked_np)\n",
    "    ax2.set_title('Masked Input', fontsize=12)\n",
    "    ax2.axis('off')\n",
    "\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    ax3.imshow(cnn_np)\n",
    "    ax3.set_title(f'CNN Output\\nPSNR: {metrics_cnn[\"psnr\"]:.2f}', fontsize=12)\n",
    "    ax3.axis('off')\n",
    "\n",
    "    ax4 = fig.add_subplot(gs[0, 3])\n",
    "    ax4.imshow(gan_np)\n",
    "    ax4.set_title(f'GAN Refined\\nPSNR: {metrics_gan[\"psnr\"]:.2f}', fontsize=12)\n",
    "    ax4.axis('off')\n",
    "\n",
    "    # Row 1: Difference/Error maps\n",
    "    diff_cnn = np.abs(original_np - cnn_np).mean(axis=2)\n",
    "    diff_gan = np.abs(original_np - gan_np).mean(axis=2)\n",
    "\n",
    "    ax5 = fig.add_subplot(gs[0, 4])\n",
    "    im5 = ax5.imshow(diff_cnn, cmap='hot', vmin=0, vmax=0.3)\n",
    "    ax5.set_title('CNN Error Map', fontsize=12)\n",
    "    ax5.axis('off')\n",
    "    plt.colorbar(im5, ax=ax5, fraction=0.046)\n",
    "\n",
    "    ax6 = fig.add_subplot(gs[0, 5])\n",
    "    im6 = ax6.imshow(diff_gan, cmap='hot', vmin=0, vmax=0.3)\n",
    "    ax6.set_title('GAN Error Map', fontsize=12)\n",
    "    ax6.axis('off')\n",
    "    plt.colorbar(im6, ax=ax6, fraction=0.046)\n",
    "\n",
    "    # Row 1: Perceptual difference\n",
    "    ax7 = fig.add_subplot(gs[0, 6:8])\n",
    "    ax7.imshow(np.abs(cnn_np - gan_np).mean(axis=2), cmap='coolwarm', vmin=-0.1, vmax=0.1)\n",
    "    ax7.set_title('CNN vs GAN Difference', fontsize=12)\n",
    "    ax7.axis('off')\n",
    "\n",
    "    # Row 2: Zoomed patches (find area with most difference)\n",
    "    hole_coords = np.where(1 - mask_np > 0)\n",
    "    if len(hole_coords[0]) > 0:\n",
    "        # Find center of masked region\n",
    "        cy, cx = hole_coords[0].mean().astype(int), hole_coords[1].mean().astype(int)\n",
    "        patch_size = 50\n",
    "\n",
    "        # Ensure patch is within bounds\n",
    "        y1 = max(0, cy - patch_size)\n",
    "        y2 = min(original_np.shape[0], cy + patch_size)\n",
    "        x1 = max(0, cx - patch_size)\n",
    "        x2 = min(original_np.shape[1], cx + patch_size)\n",
    "\n",
    "        # Extract patches\n",
    "        patches = [\n",
    "            original_np[y1:y2, x1:x2],\n",
    "            masked_np[y1:y2, x1:x2],\n",
    "            cnn_np[y1:y2, x1:x2],\n",
    "            gan_np[y1:y2, x1:x2]\n",
    "        ]\n",
    "\n",
    "        titles = ['Original Patch', 'Masked Patch', 'CNN Patch', 'GAN Patch']\n",
    "        for i, (patch, title) in enumerate(zip(patches, titles)):\n",
    "            ax = fig.add_subplot(gs[1, i])\n",
    "            ax.imshow(patch)\n",
    "            ax.set_title(title, fontsize=10)\n",
    "            ax.axis('off')\n",
    "            # Add red rectangle to show zoom area\n",
    "            if i == 0:\n",
    "                rect = plt.Rectangle((x1, y1), x2-x1, y2-y1,\n",
    "                                    fill=False, edgecolor='red', linewidth=2)\n",
    "                ax1.add_patch(rect)\n",
    "\n",
    "    # Row 2: Gradient/Edge analysis\n",
    "    def compute_edges(img):\n",
    "        gray = np.dot(img, [0.2989, 0.5870, 0.1140])\n",
    "        from scipy import ndimage\n",
    "        edges = ndimage.sobel(gray)\n",
    "        return edges\n",
    "\n",
    "    edges_original = compute_edges(original_np)\n",
    "    edges_cnn = compute_edges(cnn_np)\n",
    "    edges_gan = compute_edges(gan_np)\n",
    "\n",
    "    ax_e1 = fig.add_subplot(gs[1, 4])\n",
    "    ax_e1.imshow(edges_original, cmap='gray')\n",
    "    ax_e1.set_title('Original Edges', fontsize=10)\n",
    "    ax_e1.axis('off')\n",
    "\n",
    "    ax_e2 = fig.add_subplot(gs[1, 5])\n",
    "    ax_e2.imshow(edges_cnn, cmap='gray')\n",
    "    ax_e2.set_title('CNN Edges', fontsize=10)\n",
    "    ax_e2.axis('off')\n",
    "\n",
    "    ax_e3 = fig.add_subplot(gs[1, 6])\n",
    "    ax_e3.imshow(edges_gan, cmap='gray')\n",
    "    ax_e3.set_title('GAN Edges', fontsize=10)\n",
    "    ax_e3.axis('off')\n",
    "\n",
    "    # Row 3: Histograms\n",
    "    ax_hist = fig.add_subplot(gs[2, :4])\n",
    "    for img, label, color in [(original_np, 'Original', 'blue'),\n",
    "                              (cnn_np, 'CNN', 'orange'),\n",
    "                              (gan_np, 'GAN', 'green')]:\n",
    "        ax_hist.hist(img.flatten(), bins=50, alpha=0.5, label=label, color=color, density=True)\n",
    "    ax_hist.set_xlabel('Pixel Value')\n",
    "    ax_hist.set_ylabel('Density')\n",
    "    ax_hist.set_title('Pixel Value Distribution')\n",
    "    ax_hist.legend()\n",
    "    ax_hist.grid(True, alpha=0.3)\n",
    "\n",
    "    # Row 3: Metrics radar chart\n",
    "    ax_radar = fig.add_subplot(gs[2, 4:], projection='polar')\n",
    "\n",
    "    metrics_to_plot = ['psnr', 'ssim', 'ms_ssim']\n",
    "    if 'psnr_hole' in metrics_cnn:\n",
    "        metrics_to_plot.extend(['psnr_hole', 'ssim_hole'])\n",
    "\n",
    "    angles = np.linspace(0, 2 * np.pi, len(metrics_to_plot), endpoint=False).tolist()\n",
    "\n",
    "    # Normalize metrics for radar chart\n",
    "    cnn_values = []\n",
    "    gan_values = []\n",
    "    for metric in metrics_to_plot:\n",
    "        if metric in metrics_cnn:\n",
    "            # Normalize to 0-1 scale\n",
    "            if metric.startswith('psnr'):\n",
    "                cnn_values.append(metrics_cnn[metric] / 40)  # Assuming max PSNR ~40\n",
    "                gan_values.append(metrics_gan[metric] / 40)\n",
    "            else:\n",
    "                cnn_values.append(metrics_cnn[metric])\n",
    "                gan_values.append(metrics_gan[metric])\n",
    "\n",
    "    angles += angles[:1]\n",
    "    cnn_values += cnn_values[:1]\n",
    "    gan_values += gan_values[:1]\n",
    "\n",
    "    ax_radar.plot(angles, cnn_values, 'o-', linewidth=2, label='CNN', color='orange')\n",
    "    ax_radar.fill(angles, cnn_values, alpha=0.25, color='orange')\n",
    "    ax_radar.plot(angles, gan_values, 'o-', linewidth=2, label='GAN', color='green')\n",
    "    ax_radar.fill(angles, gan_values, alpha=0.25, color='green')\n",
    "\n",
    "    ax_radar.set_xticks(angles[:-1])\n",
    "    ax_radar.set_xticklabels(metrics_to_plot)\n",
    "    ax_radar.set_ylim(0, 1)\n",
    "    ax_radar.legend(loc='upper right')\n",
    "    ax_radar.set_title('Metrics Comparison', fontsize=12, pad=20)\n",
    "\n",
    "    # Row 4-5: Detailed metrics table\n",
    "    ax_table = fig.add_subplot(gs[3:, :])\n",
    "    ax_table.axis('tight')\n",
    "    ax_table.axis('off')\n",
    "\n",
    "    # Create metrics table\n",
    "    table_data = []\n",
    "    table_data.append(['Metric', 'CNN', 'GAN', 'Improvement', 'Best'])\n",
    "\n",
    "    for metric in metrics_cnn.keys():\n",
    "        if metric in metrics_gan:\n",
    "            cnn_val = metrics_cnn[metric]\n",
    "            gan_val = metrics_gan[metric]\n",
    "\n",
    "            # Determine if higher or lower is better\n",
    "            if metric in ['lpips', 'mae', 'mse', 'grad_diff', 'boundary_mae', 'mae_hole']:\n",
    "                improvement = cnn_val - gan_val\n",
    "                better = '↓'\n",
    "                best = 'GAN' if gan_val < cnn_val else 'CNN'\n",
    "            else:\n",
    "                improvement = gan_val - cnn_val\n",
    "                better = '↑'\n",
    "                best = 'GAN' if gan_val > cnn_val else 'CNN'\n",
    "\n",
    "            # Format values\n",
    "            if metric.startswith('time'):\n",
    "                row = [metric, f'{cnn_val:.3f}s', f'{gan_val:.3f}s',\n",
    "                      f'{improvement:+.3f}s {better}', best]\n",
    "            else:\n",
    "                row = [metric, f'{cnn_val:.4f}', f'{gan_val:.4f}',\n",
    "                      f'{improvement:+.4f} {better}', best]\n",
    "\n",
    "            table_data.append(row)\n",
    "\n",
    "    table = ax_table.table(cellText=table_data, loc='center', cellLoc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2, 1.5)\n",
    "\n",
    "    # Color code the header\n",
    "    for i in range(5):\n",
    "        table[(0, i)].set_facecolor('#40466e')\n",
    "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "    # Color code the best column\n",
    "    for i in range(1, len(table_data)):\n",
    "        if table_data[i][4] == 'GAN':\n",
    "            table[(i, 4)].set_facecolor('#90EE90')\n",
    "        else:\n",
    "            table[(i, 4)].set_facecolor('#FFB6C1')\n",
    "\n",
    "    plt.suptitle('Comprehensive Inpainting Analysis', fontsize=16, fontweight='bold', y=0.98)\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    return fig"
   ],
   "metadata": {
    "id": "Laae4Qdf3to_",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754626427675,
     "user_tz": -120,
     "elapsed": 24,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    }
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_with_statistics(model_cnn, model_gan, test_loader, num_samples=100):\n",
    "    \"\"\"Evaluate models with confidence intervals and statistical tests\"\"\"\n",
    "\n",
    "    metrics_calc = ComprehensiveMetrics()\n",
    "    results = {'cnn': [], 'gan': []}\n",
    "\n",
    "    for i, batch in enumerate(tqdm(test_loader, desc=\"Statistical Evaluation\")):\n",
    "        if i >= num_samples // test_loader.batch_size:\n",
    "            break\n",
    "\n",
    "        images = batch.to('cuda')\n",
    "        masked, masks = create_mask(images, mask_percentage=0.025)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # CNN inference\n",
    "            cnn_input = torch.cat([masked, masks[:, 0:1]], dim=1)\n",
    "            cnn_output = model_cnn(cnn_input)\n",
    "\n",
    "            # GAN inference\n",
    "            gan_output = model_gan(cnn_output, masked, masks[:, 0:1])\n",
    "\n",
    "            # Calculate metrics for each image\n",
    "            for j in range(images.shape[0]):\n",
    "                metrics_cnn = metrics_calc.calculate_all_metrics(\n",
    "                    cnn_output[j:j+1], images[j:j+1], masks[j:j+1]\n",
    "                )\n",
    "                metrics_gan = metrics_calc.calculate_all_metrics(\n",
    "                    gan_output[j:j+1], images[j:j+1], masks[j:j+1]\n",
    "                )\n",
    "\n",
    "                results['cnn'].append(metrics_cnn)\n",
    "                results['gan'].append(metrics_gan)\n",
    "\n",
    "    # Statistical analysis\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STATISTICAL ANALYSIS WITH CONFIDENCE INTERVALS\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    metric_names = results['cnn'][0].keys()\n",
    "\n",
    "    for metric in metric_names:\n",
    "        cnn_values = [r[metric] for r in results['cnn']]\n",
    "        gan_values = [r[metric] for r in results['gan']]\n",
    "\n",
    "        # Calculate statistics\n",
    "        cnn_mean = np.mean(cnn_values)\n",
    "        cnn_std = np.std(cnn_values)\n",
    "        cnn_ci = stats.t.interval(0.95, len(cnn_values)-1,\n",
    "                                  loc=cnn_mean,\n",
    "                                  scale=stats.sem(cnn_values))\n",
    "\n",
    "        gan_mean = np.mean(gan_values)\n",
    "        gan_std = np.std(gan_values)\n",
    "        gan_ci = stats.t.interval(0.95, len(gan_values)-1,\n",
    "                                  loc=gan_mean,\n",
    "                                  scale=stats.sem(gan_values))\n",
    "\n",
    "        # Paired t-test\n",
    "        t_stat, p_value = stats.ttest_rel(gan_values, cnn_values)\n",
    "\n",
    "        print(f\"\\n{metric.upper()}:\")\n",
    "        print(f\"  CNN: {cnn_mean:.4f} ± {cnn_std:.4f} (95% CI: [{cnn_ci[0]:.4f}, {cnn_ci[1]:.4f}])\")\n",
    "        print(f\"  GAN: {gan_mean:.4f} ± {gan_std:.4f} (95% CI: [{gan_ci[0]:.4f}, {gan_ci[1]:.4f}])\")\n",
    "        print(f\"  Difference: {gan_mean - cnn_mean:+.4f}\")\n",
    "        print(f\"  P-value: {p_value:.6f} {'***' if p_value < 0.001 else '**' if p_value < 0.01 else '*' if p_value < 0.05 else 'ns'}\")\n",
    "\n",
    "        # Effect size (Cohen's d)\n",
    "        cohens_d = (gan_mean - cnn_mean) / np.sqrt((cnn_std**2 + gan_std**2) / 2)\n",
    "        print(f\"  Effect size (Cohen's d): {cohens_d:.3f}\")\n",
    "\n",
    "    return results"
   ],
   "metadata": {
    "id": "IkzAcY-I3wJ4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754626432831,
     "user_tz": -120,
     "elapsed": 46,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    }
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "########################\n",
    "# USAGE\n",
    "########################\n",
    "\n",
    "# Initialize comprehensive metrics\n",
    "metrics_calculator = ComprehensiveMetrics(device='cuda')\n",
    "\n",
    "# Run statistical evaluation\n",
    "results = evaluate_with_statistics(cnn_model, generator, test_dataloader, num_samples=200)\n",
    "\n",
    "# Create advanced visualization for a sample\n",
    "sample_batch = next(iter(test_dataloader))\n",
    "sample_image = sample_batch[0:1].to('cuda')\n",
    "masked, mask = create_mask(sample_image, mask_percentage=0.025)\n",
    "\n",
    "with torch.no_grad():\n",
    "    cnn_input = torch.cat([masked, mask[:, 0:1]], dim=1)\n",
    "    cnn_output = cnn_model(cnn_input)\n",
    "    gan_output = generator(cnn_output, masked, mask[:, 0:1])\n",
    "\n",
    "    metrics_cnn = metrics_calculator.calculate_all_metrics(cnn_output, sample_image, mask)\n",
    "    metrics_gan = metrics_calculator.calculate_all_metrics(gan_output, sample_image, mask)\n",
    "\n",
    "# Create visualization\n",
    "create_advanced_visualization(\n",
    "    sample_image[0], masked[0], cnn_output[0], gan_output[0], mask,\n",
    "    metrics_cnn, metrics_gan, save_path='advanced_analysis.png'\n",
    ")\n",
    "\n",
    "print(\"Evaluation complete!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "2d9c45ad6f4d451991a5d518adc85e9f",
      "005903c471c54eca88aa982870bfbf59",
      "c586a2b5c38e47548045b336262cc951",
      "48b743f4dd1d45429d8e0444437f4516",
      "6fbb10fb9b5a44f593ff1a40bff3972a",
      "c80492ed802445aabd1dc9e4dc1e4ba0",
      "3c71479dab87477b83dfb7a8a0d7b322",
      "67dd15343e1a46468ab0a4d063e24d3c",
      "5d139fc180ca4739858ca55b70937465",
      "9209d2ab63294f2389f04db9a0572f05",
      "f0c4be38e0314842acc3536dba8ee2a0"
     ]
    },
    "id": "1izOM8J-36su",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754626917506,
     "user_tz": -120,
     "elapsed": 12536,
     "user": {
      "displayName": "Angelo",
      "userId": "01922958699684362191"
     }
    },
    "outputId": "77feaf6b-51db-4b43-b1d1-7e71abaf6fd2"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
